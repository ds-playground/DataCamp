{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building deep learning models with keras**\n",
    "\n",
    "In this chapter, you'll use the keras library to build deep learning models for both regression as well as classification! You'll learn about the Specify-Compile-Fit workflow that you can use to make predictions and by the end of this chapter, you'll have all the tools necessary to build deep neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding your data**\n",
    "\n",
    "You will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it's good to understand your data by performing some exploratory analysis.\n",
    "\n",
    "The data is pre-loaded into a pandas DataFrame called `df`. Use the `.head()` and `.describe()` methods in the IPython Shell for a quick overview of the DataFrame.\n",
    "\n",
    "The target variable you'll be predicting is `wage_per_hour`. Some of the predictor variables are binary indicators, where a value of `1` represents `True`, and `0` represents `False`.\n",
    "\n",
    "Of the `9` predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by `.describe()` will be informative here. How many binary indicator predictors are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hourly_wages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>11.36</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>6.10</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>23.25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>19.88</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>15.38</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "529          11.36      0             18               5   29       0     0   \n",
       "530           6.10      0             12              33   51       1     1   \n",
       "531          23.25      1             17              25   48       1     1   \n",
       "532          19.88      1             12              13   31       0     1   \n",
       "533          15.38      0             16              33   55       0     1   \n",
       "\n",
       "     south  manufacturing  construction  \n",
       "529      0              0             0  \n",
       "530      0              0             0  \n",
       "531      0              0             0  \n",
       "532      1              0             0  \n",
       "533      0              1             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.024064</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>13.018727</td>\n",
       "      <td>17.822097</td>\n",
       "      <td>36.833333</td>\n",
       "      <td>0.458801</td>\n",
       "      <td>0.655431</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>0.185393</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.139097</td>\n",
       "      <td>0.384360</td>\n",
       "      <td>2.615373</td>\n",
       "      <td>12.379710</td>\n",
       "      <td>11.726573</td>\n",
       "      <td>0.498767</td>\n",
       "      <td>0.475673</td>\n",
       "      <td>0.455170</td>\n",
       "      <td>0.388981</td>\n",
       "      <td>0.207375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.780000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>44.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wage_per_hour       union  education_yrs  experience_yrs         age  \\\n",
       "count     534.000000  534.000000     534.000000      534.000000  534.000000   \n",
       "mean        9.024064    0.179775      13.018727       17.822097   36.833333   \n",
       "std         5.139097    0.384360       2.615373       12.379710   11.726573   \n",
       "min         1.000000    0.000000       2.000000        0.000000   18.000000   \n",
       "25%         5.250000    0.000000      12.000000        8.000000   28.000000   \n",
       "50%         7.780000    0.000000      12.000000       15.000000   35.000000   \n",
       "75%        11.250000    0.000000      15.000000       26.000000   44.000000   \n",
       "max        44.500000    1.000000      18.000000       55.000000   64.000000   \n",
       "\n",
       "           female        marr       south  manufacturing  construction  \n",
       "count  534.000000  534.000000  534.000000     534.000000    534.000000  \n",
       "mean     0.458801    0.655431    0.292135       0.185393      0.044944  \n",
       "std      0.498767    0.475673    0.455170       0.388981      0.207375  \n",
       "min      0.000000    0.000000    0.000000       0.000000      0.000000  \n",
       "25%      0.000000    0.000000    0.000000       0.000000      0.000000  \n",
       "50%      0.000000    1.000000    0.000000       0.000000      0.000000  \n",
       "75%      1.000000    1.000000    1.000000       0.000000      0.000000  \n",
       "max      1.000000    1.000000    1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specifying a model**\n",
    "\n",
    "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\n",
    "\n",
    "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n",
    "\n",
    "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called `df`. For convenience, everything in `df` except for the target has been converted to a NumPy matrix called `predictors`. The target, `wage_per_hour`, is available as a NumPy matrix called `target`.\n",
    "\n",
    "For all exercises in this chapter, we've imported the `Sequential` model constructor, the `Dense` layer constructor, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = df.iloc[:, 1:].values\n",
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.iloc[:, 0].values\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling the model**\n",
    "\n",
    "You're now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers [here](https://keras.io/optimizers/#adam), and if you are really curious to learn more, you can read the [original paper](https://arxiv.org/abs/1412.6980v8) that introduced the Adam optimizer.\n",
    "\n",
    "In this exercise, you'll use the Adam optimizer and the mean squared error loss function. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model**\n",
    "\n",
    "You're at the most fun part. You'll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called `predictors` and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 112.1103\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 58us/step - loss: 33.9149\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 47us/step - loss: 29.9458\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 52us/step - loss: 25.8971\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 60us/step - loss: 23.7409\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 51us/step - loss: 22.1118\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 52us/step - loss: 21.7496\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 51us/step - loss: 21.4748\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 58us/step - loss: 21.3857\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 49us/step - loss: 21.1268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xc288ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 85.5178\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 45.9071     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 74.4328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 28.9703     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 14.2559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 23.4206     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 12.4489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.4502     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 9.1276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.4620    \n",
    "Epoch 6/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 20.8471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.3464     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 13.4813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.1690     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 10.7564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.1876     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 36.3705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.6074     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 19.8555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 20.9601\n",
    "Out[1]: <keras.callbacks.History at 0x7f55b44cd5c0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding your classification data**\n",
    "\n",
    "Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as `age`, `fare` and where each passenger embarked from to predict who will survive. This data is from a [tutorial on data science competitions](https://www.kaggle.com/c/titanic). Look here for descriptions of the features.\n",
    "\n",
    "The data is pre-loaded in a pandas DataFrame called `df`.\n",
    "\n",
    "It's smart to review the maximum and minimum values of each variable to ensure the data isn't misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the `.describe()` method in the IPython Shell to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic_all_numeric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1           False   \n",
       "1         1       1  38.0      1      0  71.2833     0           False   \n",
       "2         1       3  26.0      0      0   7.9250     0           False   \n",
       "3         1       1  35.0      1      0  53.1000     0           False   \n",
       "4         0       3  35.0      0      0   8.0500     1           False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.45</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass        age  sibsp  parch   fare  male age_was_missing  \\\n",
       "886         0       2  27.000000      0      0  13.00     1           False   \n",
       "887         1       1  19.000000      0      0  30.00     0           False   \n",
       "888         0       3  29.699118      1      2  23.45     0            True   \n",
       "889         1       1  26.000000      0      0  30.00     1           False   \n",
       "890         0       3  32.000000      0      0   7.75     1           False   \n",
       "\n",
       "     embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "886                        0                         0   \n",
       "887                        0                         0   \n",
       "888                        0                         0   \n",
       "889                        1                         0   \n",
       "890                        0                         1   \n",
       "\n",
       "     embarked_from_southampton  \n",
       "886                          1  \n",
       "887                          1  \n",
       "888                          1  \n",
       "889                          0  \n",
       "890                          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.002015</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208   \n",
       "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429   \n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400   \n",
       "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200   \n",
       "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000   \n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200   \n",
       "\n",
       "             male  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "count  891.000000               891.000000                891.000000   \n",
       "mean     0.647587                 0.188552                  0.086420   \n",
       "std      0.477990                 0.391372                  0.281141   \n",
       "min      0.000000                 0.000000                  0.000000   \n",
       "25%      0.000000                 0.000000                  0.000000   \n",
       "50%      1.000000                 0.000000                  0.000000   \n",
       "75%      1.000000                 0.000000                  0.000000   \n",
       "max      1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       embarked_from_southampton  \n",
       "count                 891.000000  \n",
       "mean                    0.722783  \n",
       "std                     0.447876  \n",
       "min                     0.000000  \n",
       "25%                     0.000000  \n",
       "50%                     1.000000  \n",
       "75%                     1.000000  \n",
       "max                     1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last steps in classification models**\n",
    "\n",
    "You'll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called `df`. You'll take information about the passengers and predict which ones survived.\n",
    "\n",
    "The predictive variables are stored in a NumPy array `predictors`. The target to predict is in `df.survived`, though you'll have to manipulate it for keras. The number of predictive features is stored in `n_cols`.\n",
    "\n",
    "Here, you'll use the `'sgd'` optimizer, which stands for [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). You'll learn more about this in the next chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = df.iloc[:, 1:].values\n",
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.survived\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols = predictors.shape[1]\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpifkdt8fv'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'df':      survived  pclass        age  sibsp  parch      fare  male  \\\n",
    " 0           0       3  22.000000      1      0    7.2500     1   \n",
    " 1           1       1  38.000000      1      0   71.2833     0   \n",
    " 2           1       3  26.000000      0      0    7.9250     0   \n",
    " 3           1       1  35.000000      1      0   53.1000     0   \n",
    " 4           0       3  35.000000      0      0    8.0500     1   \n",
    " 5           0       3  29.699118      0      0    8.4583     1   \n",
    " 6           0       1  54.000000      0      0   51.8625     1   \n",
    " 7           0       3   2.000000      3      1   21.0750     1   \n",
    " 8           1       3  27.000000      0      2   11.1333     0   \n",
    " 9           1       2  14.000000      1      0   30.0708     0   \n",
    " 10          1       3   4.000000      1      1   16.7000     0   \n",
    " 11          1       1  58.000000      0      0   26.5500     0   \n",
    " 12          0       3  20.000000      0      0    8.0500     1   \n",
    " 13          0       3  39.000000      1      5   31.2750     1   \n",
    " 14          0       3  14.000000      0      0    7.8542     0   \n",
    " 15          1       2  55.000000      0      0   16.0000     0   \n",
    " 16          0       3   2.000000      4      1   29.1250     1   \n",
    " 17          1       2  29.699118      0      0   13.0000     1   \n",
    " 18          0       3  31.000000      1      0   18.0000     0   \n",
    " 19          1       3  29.699118      0      0    7.2250     0   \n",
    " 20          0       2  35.000000      0      0   26.0000     1   \n",
    " 21          1       2  34.000000      0      0   13.0000     1   \n",
    " 22          1       3  15.000000      0      0    8.0292     0   \n",
    " 23          1       1  28.000000      0      0   35.5000     1   \n",
    " 24          0       3   8.000000      3      1   21.0750     0   \n",
    " 25          1       3  38.000000      1      5   31.3875     0   \n",
    " 26          0       3  29.699118      0      0    7.2250     1   \n",
    " 27          0       1  19.000000      3      2  263.0000     1   \n",
    " 28          1       3  29.699118      0      0    7.8792     0   \n",
    " 29          0       3  29.699118      0      0    7.8958     1   \n",
    " ..        ...     ...        ...    ...    ...       ...   ...   \n",
    " 861         0       2  21.000000      1      0   11.5000     1   \n",
    " 862         1       1  48.000000      0      0   25.9292     0   \n",
    " 863         0       3  29.699118      8      2   69.5500     0   \n",
    " 864         0       2  24.000000      0      0   13.0000     1   \n",
    " 865         1       2  42.000000      0      0   13.0000     0   \n",
    " 866         1       2  27.000000      1      0   13.8583     0   \n",
    " 867         0       1  31.000000      0      0   50.4958     1   \n",
    " 868         0       3  29.699118      0      0    9.5000     1   \n",
    " 869         1       3   4.000000      1      1   11.1333     1   \n",
    " 870         0       3  26.000000      0      0    7.8958     1   \n",
    " 871         1       1  47.000000      1      1   52.5542     0   \n",
    " 872         0       1  33.000000      0      0    5.0000     1   \n",
    " 873         0       3  47.000000      0      0    9.0000     1   \n",
    " 874         1       2  28.000000      1      0   24.0000     0   \n",
    " 875         1       3  15.000000      0      0    7.2250     0   \n",
    " 876         0       3  20.000000      0      0    9.8458     1   \n",
    " 877         0       3  19.000000      0      0    7.8958     1   \n",
    " 878         0       3  29.699118      0      0    7.8958     1   \n",
    " 879         1       1  56.000000      0      1   83.1583     0   \n",
    " 880         1       2  25.000000      0      1   26.0000     0   \n",
    " 881         0       3  33.000000      0      0    7.8958     1   \n",
    " 882         0       3  22.000000      0      0   10.5167     0   \n",
    " 883         0       2  28.000000      0      0   10.5000     1   \n",
    " 884         0       3  25.000000      0      0    7.0500     1   \n",
    " 885         0       3  39.000000      0      5   29.1250     0   \n",
    " 886         0       2  27.000000      0      0   13.0000     1   \n",
    " 887         1       1  19.000000      0      0   30.0000     0   \n",
    " 888         0       3  29.699118      1      2   23.4500     0   \n",
    " 889         1       1  26.000000      0      0   30.0000     1   \n",
    " 890         0       3  32.000000      0      0    7.7500     1   \n",
    " \n",
    "      age_was_missing  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
    " 0                  0                        0                         0   \n",
    " 1                  0                        1                         0   \n",
    " 2                  0                        0                         0   \n",
    " 3                  0                        0                         0   \n",
    " 4                  0                        0                         0   \n",
    " 5                  1                        0                         1   \n",
    " 6                  0                        0                         0   \n",
    " 7                  0                        0                         0   \n",
    " 8                  0                        0                         0   \n",
    " 9                  0                        1                         0   \n",
    " 10                 0                        0                         0   \n",
    " 11                 0                        0                         0   \n",
    " 12                 0                        0                         0   \n",
    " 13                 0                        0                         0   \n",
    " 14                 0                        0                         0   \n",
    " 15                 0                        0                         0   \n",
    " 16                 0                        0                         1   \n",
    " 17                 1                        0                         0   \n",
    " 18                 0                        0                         0   \n",
    " 19                 1                        1                         0   \n",
    " 20                 0                        0                         0   \n",
    " 21                 0                        0                         0   \n",
    " 22                 0                        0                         1   \n",
    " 23                 0                        0                         0   \n",
    " 24                 0                        0                         0   \n",
    " 25                 0                        0                         0   \n",
    " 26                 1                        1                         0   \n",
    " 27                 0                        0                         0   \n",
    " 28                 1                        0                         1   \n",
    " 29                 1                        0                         0   \n",
    " ..               ...                      ...                       ...   \n",
    " 861                0                        0                         0   \n",
    " 862                0                        0                         0   \n",
    " 863                1                        0                         0   \n",
    " 864                0                        0                         0   \n",
    " 865                0                        0                         0   \n",
    " 866                0                        1                         0   \n",
    " 867                0                        0                         0   \n",
    " 868                1                        0                         0   \n",
    " 869                0                        0                         0   \n",
    " 870                0                        0                         0   \n",
    " 871                0                        0                         0   \n",
    " 872                0                        0                         0   \n",
    " 873                0                        0                         0   \n",
    " 874                0                        1                         0   \n",
    " 875                0                        1                         0   \n",
    " 876                0                        0                         0   \n",
    " 877                0                        0                         0   \n",
    " 878                1                        0                         0   \n",
    " 879                0                        1                         0   \n",
    " 880                0                        0                         0   \n",
    " 881                0                        0                         0   \n",
    " 882                0                        0                         0   \n",
    " 883                0                        0                         0   \n",
    " 884                0                        0                         0   \n",
    " 885                0                        0                         1   \n",
    " 886                0                        0                         0   \n",
    " 887                0                        0                         0   \n",
    " 888                1                        0                         0   \n",
    " 889                0                        1                         0   \n",
    " 890                0                        0                         1   \n",
    " \n",
    "      embarked_from_southampton  \n",
    " 0                            1  \n",
    " 1                            0  \n",
    " 2                            1  \n",
    " 3                            1  \n",
    " 4                            1  \n",
    " 5                            0  \n",
    " 6                            1  \n",
    " 7                            1  \n",
    " 8                            1  \n",
    " 9                            0  \n",
    " 10                           1  \n",
    " 11                           1  \n",
    " 12                           1  \n",
    " 13                           1  \n",
    " 14                           1  \n",
    " 15                           1  \n",
    " 16                           0  \n",
    " 17                           1  \n",
    " 18                           1  \n",
    " 19                           0  \n",
    " 20                           1  \n",
    " 21                           1  \n",
    " 22                           0  \n",
    " 23                           1  \n",
    " 24                           1  \n",
    " 25                           1  \n",
    " 26                           0  \n",
    " 27                           1  \n",
    " 28                           0  \n",
    " 29                           1  \n",
    " ..                         ...  \n",
    " 861                          1  \n",
    " 862                          1  \n",
    " 863                          1  \n",
    " 864                          1  \n",
    " 865                          1  \n",
    " 866                          0  \n",
    " 867                          1  \n",
    " 868                          1  \n",
    " 869                          1  \n",
    " 870                          1  \n",
    " 871                          1  \n",
    " 872                          1  \n",
    " 873                          1  \n",
    " 874                          0  \n",
    " 875                          0  \n",
    " 876                          1  \n",
    " 877                          1  \n",
    " 878                          1  \n",
    " 879                          0  \n",
    " 880                          1  \n",
    " 881                          1  \n",
    " 882                          1  \n",
    " 883                          1  \n",
    " 884                          1  \n",
    " 885                          0  \n",
    " 886                          1  \n",
    " 887                          1  \n",
    " 888                          1  \n",
    " 889                          0  \n",
    " 890                          0  \n",
    " \n",
    " [891 rows x 11 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55c3a36400>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'n_cols': 10,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'predictors': array([[  3.        ,  22.        ,   1.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        [  1.        ,  38.        ,   1.        , ...,   1.        ,\n",
    "           0.        ,   0.        ],\n",
    "        [  3.        ,  26.        ,   0.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        ..., \n",
    "        [  3.        ,  29.69911765,   1.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        [  1.        ,  26.        ,   0.        , ...,   1.        ,\n",
    "           0.        ,   0.        ],\n",
    "        [  3.        ,  32.        ,   0.        , ...,   0.        ,\n",
    "           1.        ,   0.        ]]),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55c3a36400>,\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s 374us/step - loss: 3.6592 - acc: 0.5645\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 44us/step - loss: 2.0811 - acc: 0.6038\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 43us/step - loss: 1.0067 - acc: 0.6364\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 46us/step - loss: 0.7448 - acc: 0.6588\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.6774 - acc: 0.6824\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.6267 - acc: 0.6891\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - ETA: 0s - loss: 0.7115 - acc: 0.656 - 0s 38us/step - loss: 0.6355 - acc: 0.6700\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.5934 - acc: 0.6869\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.5968 - acc: 0.6869\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.5870 - acc: 0.7015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd63ad30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))  \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 2.2860 - acc: 0.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 2.7017 - acc: 0.5771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 2.0223 - acc: 0.6150     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 1.3087 - acc: 0.4062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 1.1448 - acc: 0.6229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "864/891 [============================>.] - ETA: 0s - loss: 1.1202 - acc: 0.6169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 1.1082 - acc: 0.6184     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 1.2949 - acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "416/891 [=============>................] - ETA: 0s - loss: 0.9214 - acc: 0.6154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/891 [=========================>....] - ETA: 0s - loss: 0.9359 - acc: 0.6138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.9109 - acc: 0.6173     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.8078 - acc: 0.5625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "576/891 [==================>...........] - ETA: 0s - loss: 0.7991 - acc: 0.6597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7541 - acc: 0.6566     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7259 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/891 [======================>.......] - ETA: 0s - loss: 0.7659 - acc: 0.6605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7704 - acc: 0.6465     \n",
    "Epoch 6/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.5819 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "448/891 [==============>...............] - ETA: 0s - loss: 0.7569 - acc: 0.6540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7122 - acc: 0.6532     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7170 - acc: 0.6875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 0.6419 - acc: 0.6708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6810 - acc: 0.6813     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.5142 - acc: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "576/891 [==================>...........] - ETA: 0s - loss: 0.6646 - acc: 0.6858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6461 - acc: 0.6790     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7385 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "416/891 [=============>................] - ETA: 0s - loss: 0.6555 - acc: 0.6587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/891 [=========================>....] - ETA: 0s - loss: 0.6653 - acc: 0.6675\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6660 - acc: 0.6723     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.9764 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "352/891 [==========>...................] - ETA: 0s - loss: 0.6541 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/891 [=====================>........] - ETA: 0s - loss: 0.6135 - acc: 0.6771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6132 - acc: 0.6745\n",
    "Out[2]: <keras.callbacks.History at 0x7f55b4c83fd0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making predictions**\n",
    "\n",
    "The trained network from your previous coding exercise is now stored as `model`. New data to make predictions is stored in a NumPy array as `pred_data`. Use `model` to make predictions on your new data.\n",
    "\n",
    "In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = np.array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "        [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "        [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "        [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "        [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "        [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "        [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "        [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "        [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "        [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "        [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "        [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "        [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "        [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "        [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "        [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "        [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "        [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "        [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "        [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "        [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "        [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "        [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "        [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "        [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "        [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "        [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "        [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "        [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "        [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "        [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "        [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "        [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "        [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "        [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "        [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "        [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "        [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "        [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "        [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "        [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "        [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "        [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "        [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "        [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]])\n",
    "train_size = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " 'X': array([[3, 22.0, 1, ..., 0, 0, 1],\n",
    "        [1, 38.0, 1, ..., 1, 0, 0],\n",
    "        [3, 26.0, 0, ..., 0, 0, 1],\n",
    "        ..., \n",
    "        [3, 29.69911764705882, 1, ..., 0, 0, 1],\n",
    "        [1, 26.0, 0, ..., 1, 0, 0],\n",
    "        [3, 32.0, 0, ..., 0, 1, 0]], dtype=object),\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpd9pr98uj'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'data':      survived  pclass        age  sibsp  parch      fare  male  \\\n",
    " 0           0       3  22.000000      1      0    7.2500     1   \n",
    " 1           1       1  38.000000      1      0   71.2833     0   \n",
    " 2           1       3  26.000000      0      0    7.9250     0   \n",
    " 3           1       1  35.000000      1      0   53.1000     0   \n",
    " 4           0       3  35.000000      0      0    8.0500     1   \n",
    " 5           0       3  29.699118      0      0    8.4583     1   \n",
    " 6           0       1  54.000000      0      0   51.8625     1   \n",
    " 7           0       3   2.000000      3      1   21.0750     1   \n",
    " 8           1       3  27.000000      0      2   11.1333     0   \n",
    " 9           1       2  14.000000      1      0   30.0708     0   \n",
    " 10          1       3   4.000000      1      1   16.7000     0   \n",
    " 11          1       1  58.000000      0      0   26.5500     0   \n",
    " 12          0       3  20.000000      0      0    8.0500     1   \n",
    " 13          0       3  39.000000      1      5   31.2750     1   \n",
    " 14          0       3  14.000000      0      0    7.8542     0   \n",
    " 15          1       2  55.000000      0      0   16.0000     0   \n",
    " 16          0       3   2.000000      4      1   29.1250     1   \n",
    " 17          1       2  29.699118      0      0   13.0000     1   \n",
    " 18          0       3  31.000000      1      0   18.0000     0   \n",
    " 19          1       3  29.699118      0      0    7.2250     0   \n",
    " 20          0       2  35.000000      0      0   26.0000     1   \n",
    " 21          1       2  34.000000      0      0   13.0000     1   \n",
    " 22          1       3  15.000000      0      0    8.0292     0   \n",
    " 23          1       1  28.000000      0      0   35.5000     1   \n",
    " 24          0       3   8.000000      3      1   21.0750     0   \n",
    " 25          1       3  38.000000      1      5   31.3875     0   \n",
    " 26          0       3  29.699118      0      0    7.2250     1   \n",
    " 27          0       1  19.000000      3      2  263.0000     1   \n",
    " 28          1       3  29.699118      0      0    7.8792     0   \n",
    " 29          0       3  29.699118      0      0    7.8958     1   \n",
    " ..        ...     ...        ...    ...    ...       ...   ...   \n",
    " 861         0       2  21.000000      1      0   11.5000     1   \n",
    " 862         1       1  48.000000      0      0   25.9292     0   \n",
    " 863         0       3  29.699118      8      2   69.5500     0   \n",
    " 864         0       2  24.000000      0      0   13.0000     1   \n",
    " 865         1       2  42.000000      0      0   13.0000     0   \n",
    " 866         1       2  27.000000      1      0   13.8583     0   \n",
    " 867         0       1  31.000000      0      0   50.4958     1   \n",
    " 868         0       3  29.699118      0      0    9.5000     1   \n",
    " 869         1       3   4.000000      1      1   11.1333     1   \n",
    " 870         0       3  26.000000      0      0    7.8958     1   \n",
    " 871         1       1  47.000000      1      1   52.5542     0   \n",
    " 872         0       1  33.000000      0      0    5.0000     1   \n",
    " 873         0       3  47.000000      0      0    9.0000     1   \n",
    " 874         1       2  28.000000      1      0   24.0000     0   \n",
    " 875         1       3  15.000000      0      0    7.2250     0   \n",
    " 876         0       3  20.000000      0      0    9.8458     1   \n",
    " 877         0       3  19.000000      0      0    7.8958     1   \n",
    " 878         0       3  29.699118      0      0    7.8958     1   \n",
    " 879         1       1  56.000000      0      1   83.1583     0   \n",
    " 880         1       2  25.000000      0      1   26.0000     0   \n",
    " 881         0       3  33.000000      0      0    7.8958     1   \n",
    " 882         0       3  22.000000      0      0   10.5167     0   \n",
    " 883         0       2  28.000000      0      0   10.5000     1   \n",
    " 884         0       3  25.000000      0      0    7.0500     1   \n",
    " 885         0       3  39.000000      0      5   29.1250     0   \n",
    " 886         0       2  27.000000      0      0   13.0000     1   \n",
    " 887         1       1  19.000000      0      0   30.0000     0   \n",
    " 888         0       3  29.699118      1      2   23.4500     0   \n",
    " 889         1       1  26.000000      0      0   30.0000     1   \n",
    " 890         0       3  32.000000      0      0    7.7500     1   \n",
    " \n",
    "     age_was_missing  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
    " 0             False                        0                         0   \n",
    " 1             False                        1                         0   \n",
    " 2             False                        0                         0   \n",
    " 3             False                        0                         0   \n",
    " 4             False                        0                         0   \n",
    " 5              True                        0                         1   \n",
    " 6             False                        0                         0   \n",
    " 7             False                        0                         0   \n",
    " 8             False                        0                         0   \n",
    " 9             False                        1                         0   \n",
    " 10            False                        0                         0   \n",
    " 11            False                        0                         0   \n",
    " 12            False                        0                         0   \n",
    " 13            False                        0                         0   \n",
    " 14            False                        0                         0   \n",
    " 15            False                        0                         0   \n",
    " 16            False                        0                         1   \n",
    " 17             True                        0                         0   \n",
    " 18            False                        0                         0   \n",
    " 19             True                        1                         0   \n",
    " 20            False                        0                         0   \n",
    " 21            False                        0                         0   \n",
    " 22            False                        0                         1   \n",
    " 23            False                        0                         0   \n",
    " 24            False                        0                         0   \n",
    " 25            False                        0                         0   \n",
    " 26             True                        1                         0   \n",
    " 27            False                        0                         0   \n",
    " 28             True                        0                         1   \n",
    " 29             True                        0                         0   \n",
    " ..              ...                      ...                       ...   \n",
    " 861           False                        0                         0   \n",
    " 862           False                        0                         0   \n",
    " 863            True                        0                         0   \n",
    " 864           False                        0                         0   \n",
    " 865           False                        0                         0   \n",
    " 866           False                        1                         0   \n",
    " 867           False                        0                         0   \n",
    " 868            True                        0                         0   \n",
    " 869           False                        0                         0   \n",
    " 870           False                        0                         0   \n",
    " 871           False                        0                         0   \n",
    " 872           False                        0                         0   \n",
    " 873           False                        0                         0   \n",
    " 874           False                        1                         0   \n",
    " 875           False                        1                         0   \n",
    " 876           False                        0                         0   \n",
    " 877           False                        0                         0   \n",
    " 878            True                        0                         0   \n",
    " 879           False                        1                         0   \n",
    " 880           False                        0                         0   \n",
    " 881           False                        0                         0   \n",
    " 882           False                        0                         0   \n",
    " 883           False                        0                         0   \n",
    " 884           False                        0                         0   \n",
    " 885           False                        0                         1   \n",
    " 886           False                        0                         0   \n",
    " 887           False                        0                         0   \n",
    " 888            True                        0                         0   \n",
    " 889           False                        1                         0   \n",
    " 890           False                        0                         1   \n",
    " \n",
    "      embarked_from_southampton  \n",
    " 0                            1  \n",
    " 1                            0  \n",
    " 2                            1  \n",
    " 3                            1  \n",
    " 4                            1  \n",
    " 5                            0  \n",
    " 6                            1  \n",
    " 7                            1  \n",
    " 8                            1  \n",
    " 9                            0  \n",
    " 10                           1  \n",
    " 11                           1  \n",
    " 12                           1  \n",
    " 13                           1  \n",
    " 14                           1  \n",
    " 15                           1  \n",
    " 16                           0  \n",
    " 17                           1  \n",
    " 18                           1  \n",
    " 19                           0  \n",
    " 20                           1  \n",
    " 21                           1  \n",
    " 22                           0  \n",
    " 23                           1  \n",
    " 24                           1  \n",
    " 25                           1  \n",
    " 26                           0  \n",
    " 27                           1  \n",
    " 28                           0  \n",
    " 29                           1  \n",
    " ..                         ...  \n",
    " 861                          1  \n",
    " 862                          1  \n",
    " 863                          1  \n",
    " 864                          1  \n",
    " 865                          1  \n",
    " 866                          0  \n",
    " 867                          1  \n",
    " 868                          1  \n",
    " 869                          1  \n",
    " 870                          1  \n",
    " 871                          1  \n",
    " 872                          1  \n",
    " 873                          1  \n",
    " 874                          0  \n",
    " 875                          0  \n",
    " 876                          1  \n",
    " 877                          1  \n",
    " 878                          1  \n",
    " 879                          0  \n",
    " 880                          1  \n",
    " 881                          1  \n",
    " 882                          1  \n",
    " 883                          1  \n",
    " 884                          1  \n",
    " 885                          0  \n",
    " 886                          1  \n",
    " 887                          1  \n",
    " 888                          1  \n",
    " 889                          0  \n",
    " 890                          0  \n",
    " \n",
    " [891 rows x 11 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55b790e400>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'n_cols': 10,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'pred_data': array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "        [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "        [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "        [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "        [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "        [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "        [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "        [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "        [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "        [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "        [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "        [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "        [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "        [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "        [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "        [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "        [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "        [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "        [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "        [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "        [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "        [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "        [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "        [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "        [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "        [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "        [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "        [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "        [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "        [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "        [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "        [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "        [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "        [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "        [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "        [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "        [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "        [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "        [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "        [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "        [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "        [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "        [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "        [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "        [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]], dtype=object),\n",
    " 'predictors': array([[3, 22.0, 1, ..., 0, 0, 1],\n",
    "        [1, 38.0, 1, ..., 1, 0, 0],\n",
    "        [3, 26.0, 0, ..., 0, 0, 1],\n",
    "        ..., \n",
    "        [3, 31.0, 0, ..., 0, 0, 1],\n",
    "        [3, 30.0, 0, ..., 1, 0, 0],\n",
    "        [3, 30.0, 1, ..., 0, 0, 1]], dtype=object),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55b790e400>,\n",
    " 'target': array([[ 1.,  0.],\n",
    "        [ 0.,  1.],\n",
    "        [ 0.,  1.],\n",
    "        ..., \n",
    "        [ 0.,  1.],\n",
    "        [ 1.,  0.],\n",
    "        [ 1.,  0.]]),\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>,\n",
    " 'train_size': 800}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s 384us/step - loss: 2.4199 - acc: 0.6117\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 46us/step - loss: 1.3396 - acc: 0.6263\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.8734 - acc: 0.6521\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 46us/step - loss: 0.6267 - acc: 0.6846\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.6934 - acc: 0.6622\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.6203 - acc: 0.6880\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 46us/step - loss: 0.6466 - acc: 0.7015\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.5927 - acc: 0.7160\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.6081 - acc: 0.7037\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 46us/step - loss: 0.6047 - acc: 0.7003\n",
      "[ 0.25012502  0.40163684  0.78297651  0.45193568  0.19434859  0.1661966\n",
      "  0.06621139  0.36964989  0.18794109  0.62873846  0.22564894  0.31123096\n",
      "  0.18415399  0.46918815  0.17633739  0.11485128  0.32327121  0.52741623\n",
      "  0.06597202  0.47872633  0.74648315  0.23009977  0.07011615  0.34685138\n",
      "  0.44115621  0.13537169  0.63130081  0.60413164  0.14742158  0.70917755\n",
      "  0.41567922  0.4863759   0.11911269  0.26542228  0.32156935  0.72312307\n",
      "  0.29876915  0.1707962   0.6375615   0.48318022  0.2981326   0.40517363\n",
      "  0.52693111  0.08635563  0.34042019  0.08000183  0.46038783  0.09865643\n",
      "  0.56596565  0.74897349  0.4439185   0.00870668  0.48792735  0.62628299\n",
      "  0.30070958  0.3913686   0.91246921  0.29569089  0.4075892   0.11911269\n",
      "  0.11206704  0.31883672  0.35354686  0.46628568  0.35717389  0.20326027\n",
      "  0.28769943  0.62971503  0.19818307  0.4746227   0.2258331   0.590478\n",
      "  0.12568259  0.06746289  0.39388728  0.35288563  0.32396197  0.30679551\n",
      "  0.16806805  0.7251417   0.51990169  0.14884497  0.3695862   0.27353621\n",
      "  0.2198953   0.38317487  0.32946098  0.5854556   0.36363447  0.54620564\n",
      "  0.13921559]\n"
     ]
    }
   ],
   "source": [
    "# Specify, compile, and fit the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 1.9889 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 2.1642 - acc: 0.5878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 2.0926 - acc: 0.5850     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.9758 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 1.2621 - acc: 0.5789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 1.2150 - acc: 0.5825     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7413 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.9169 - acc: 0.6293\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.9013 - acc: 0.6225     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.8295 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6976 - acc: 0.6335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.7067 - acc: 0.6350     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7734 - acc: 0.5312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 0.7101 - acc: 0.6771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6920 - acc: 0.6775     \n",
    "Epoch 6/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7822 - acc: 0.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6394 - acc: 0.6705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6375 - acc: 0.6687     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.5666 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "640/800 [=======================>......] - ETA: 0s - loss: 0.6170 - acc: 0.6734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6172 - acc: 0.6812     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.6544 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "384/800 [=============>................] - ETA: 0s - loss: 0.6322 - acc: 0.6693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6181 - acc: 0.6837     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7973 - acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6325 - acc: 0.6634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6183 - acc: 0.6737     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.5755 - acc: 0.6875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 0.6201 - acc: 0.6860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6142 - acc: 0.6900     \n",
    "[ 0.18310049  0.39257559  0.90957892  0.43802381  0.19449668  0.16226816\n",
    "  0.04776059  0.36320636  0.13548838  0.59659457  0.21850654  0.24186897\n",
    "  0.14519456  0.54658085  0.1703662   0.09650292  0.3025246   0.50268072\n",
    "  0.06854767  0.53118235  0.66011029  0.21718189  0.05112241  0.28982982\n",
    "  0.58332694  0.18461925  0.58715367  0.68001962  0.19473749  0.59099054\n",
    "  0.44369563  0.56261528  0.1525811   0.25684106  0.31957388  0.68460965\n",
    "  0.2890453   0.19117369  0.59348351  0.42708611  0.29303789  0.36315283\n",
    "  0.45844418  0.09916571  0.334088    0.07964922  0.56860393  0.10694957\n",
    "  0.50475955  0.78340077  0.54485911  0.00884945  0.49051741  0.63139898\n",
    "  0.25409591  0.3637715   0.93466604  0.15533349  0.3382667   0.1525811\n",
    "  0.15455711  0.32354257  0.20677377  0.57712185  0.29516548  0.13900402\n",
    "  0.28757289  0.58676606  0.21134241  0.50062877  0.21862327  0.52192533\n",
    "  0.10416862  0.0638297   0.38311687  0.35487503  0.31126469  0.3045193\n",
    "  0.18921892  0.61726612  0.47175369  0.14496818  0.34283844  0.21893513\n",
    "  0.21895605  0.26417106  0.26272085  0.54475427  0.39032492  0.49731082\n",
    "  0.15875942]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
