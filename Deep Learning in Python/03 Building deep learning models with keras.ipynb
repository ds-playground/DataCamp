{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building deep learning models with keras**\n",
    "\n",
    "In this chapter, you'll use the keras library to build deep learning models for both regression as well as classification! You'll learn about the Specify-Compile-Fit workflow that you can use to make predictions and by the end of this chapter, you'll have all the tools necessary to build deep neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding your data**\n",
    "\n",
    "You will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it's good to understand your data by performing some exploratory analysis.\n",
    "\n",
    "The data is pre-loaded into a pandas DataFrame called `df`. Use the `.head()` and `.describe()` methods in the IPython Shell for a quick overview of the DataFrame.\n",
    "\n",
    "The target variable you'll be predicting is `wage_per_hour`. Some of the predictor variables are binary indicators, where a value of `1` represents `True`, and `0` represents `False`.\n",
    "\n",
    "Of the `9` predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by `.describe()` will be informative here. How many binary indicator predictors are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specifying a model**\n",
    "\n",
    "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\n",
    "\n",
    "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n",
    "\n",
    "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called `df`. For convenience, everything in `df` except for the target has been converted to a NumPy matrix called `predictors`. The target, `wage_per_hour`, is available as a NumPy matrix called `target`.\n",
    "\n",
    "For all exercises in this chapter, we've imported the `Sequential` model constructor, the `Dense` layer constructor, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vara()', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpeidmidax'],\n",
    " '_i': 'vara()',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vara()', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'df':      wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
    " 0             5.10      0              8              21   35       1     1   \n",
    " 1             4.95      0              9              42   57       1     1   \n",
    " 2             6.67      0             12               1   19       0     0   \n",
    " 3             4.00      0             12               4   22       0     0   \n",
    " 4             7.50      0             12              17   35       0     1   \n",
    " 5            13.07      1             13               9   28       0     0   \n",
    " 6             4.45      0             10              27   43       0     0   \n",
    " 7            19.47      0             12               9   27       0     0   \n",
    " 8            13.28      0             16              11   33       0     1   \n",
    " 9             8.75      0             12               9   27       0     0   \n",
    " 10           11.35      1             12              17   35       0     1   \n",
    " 11           11.50      1             12              19   37       0     0   \n",
    " 12            6.50      0              8              27   41       0     1   \n",
    " 13            6.25      1              9              30   45       0     0   \n",
    " 14           19.98      0              9              29   44       0     1   \n",
    " 15            7.30      0             12              37   55       0     1   \n",
    " 16            8.00      0              7              44   57       0     1   \n",
    " 17           22.20      1             12              26   44       0     1   \n",
    " 18            3.65      0             11              16   33       0     0   \n",
    " 19           20.55      0             12              33   51       0     1   \n",
    " 20            5.71      1             12              16   34       1     1   \n",
    " 21            7.00      1              7              42   55       0     1   \n",
    " 22            3.75      0             12               9   27       0     0   \n",
    " 23            4.50      0             11              14   31       0     1   \n",
    " 24            9.56      0             12              23   41       0     1   \n",
    " 25            5.75      0              6              45   57       0     1   \n",
    " 26            9.36      0             12               8   26       0     1   \n",
    " 27            6.50      0             10              30   46       0     1   \n",
    " 28            3.35      0             12               8   26       1     1   \n",
    " 29            4.75      0             12               8   26       0     1   \n",
    " ..             ...    ...            ...             ...  ...     ...   ...   \n",
    " 504          11.25      0             17              10   33       1     0   \n",
    " 505           6.67      1             16              10   32       1     0   \n",
    " 506           8.00      0             16              17   39       1     1   \n",
    " 507          18.16      0             18               7   31       0     1   \n",
    " 508          12.00      0             16              14   36       1     1   \n",
    " 509           8.89      1             16              22   44       1     1   \n",
    " 510           9.50      0             17              14   37       1     1   \n",
    " 511          13.65      0             16              11   33       0     1   \n",
    " 512          12.00      1             18              23   47       0     1   \n",
    " 513          15.00      1             12              39   57       0     1   \n",
    " 514          12.67      0             16              15   37       0     1   \n",
    " 515           7.38      0             14              15   35       1     0   \n",
    " 516          15.56      0             16              10   32       0     0   \n",
    " 517           7.45      0             12              25   43       1     0   \n",
    " 518           6.25      0             14              12   32       1     1   \n",
    " 519           6.25      0             16               7   29       1     1   \n",
    " 520           9.37      1             17               7   30       0     1   \n",
    " 521          22.50      0             16              17   39       0     1   \n",
    " 522           7.50      1             16              10   32       0     1   \n",
    " 523           7.00      0             17               2   25       0     1   \n",
    " 524           5.75      1              9              34   49       1     1   \n",
    " 525           7.67      0             15              11   32       1     1   \n",
    " 526          12.50      0             15              10   31       0     0   \n",
    " 527          16.00      0             12              12   30       0     1   \n",
    " 528          11.79      1             16               6   28       1     0   \n",
    " 529          11.36      0             18               5   29       0     0   \n",
    " 530           6.10      0             12              33   51       1     1   \n",
    " 531          23.25      1             17              25   48       1     1   \n",
    " 532          19.88      1             12              13   31       0     1   \n",
    " 533          15.38      0             16              33   55       0     1   \n",
    " \n",
    "      south  manufacturing  construction  \n",
    " 0        0              1             0  \n",
    " 1        0              1             0  \n",
    " 2        0              1             0  \n",
    " 3        0              0             0  \n",
    " 4        0              0             0  \n",
    " 5        0              0             0  \n",
    " 6        1              0             0  \n",
    " 7        0              0             0  \n",
    " 8        0              1             0  \n",
    " 9        0              0             0  \n",
    " 10       0              0             0  \n",
    " 11       0              1             0  \n",
    " 12       1              0             0  \n",
    " 13       1              0             0  \n",
    " 14       1              0             0  \n",
    " 15       0              0             1  \n",
    " 16       1              0             0  \n",
    " 17       0              1             0  \n",
    " 18       0              0             0  \n",
    " 19       0              0             0  \n",
    " 20       0              1             0  \n",
    " 21       0              1             0  \n",
    " 22       0              0             0  \n",
    " 23       1              0             0  \n",
    " 24       0              0             0  \n",
    " 25       1              1             0  \n",
    " 26       0              1             0  \n",
    " 27       0              0             0  \n",
    " 28       0              1             0  \n",
    " 29       0              0             0  \n",
    " ..     ...            ...           ...  \n",
    " 504      0              0             0  \n",
    " 505      0              0             0  \n",
    " 506      0              0             0  \n",
    " 507      0              0             0  \n",
    " 508      0              0             0  \n",
    " 509      0              0             0  \n",
    " 510      0              0             0  \n",
    " 511      0              0             0  \n",
    " 512      0              0             0  \n",
    " 513      0              0             0  \n",
    " 514      0              0             0  \n",
    " 515      0              0             0  \n",
    " 516      0              0             0  \n",
    " 517      1              0             0  \n",
    " 518      0              0             0  \n",
    " 519      1              0             0  \n",
    " 520      0              0             0  \n",
    " 521      0              1             0  \n",
    " 522      0              0             0  \n",
    " 523      1              0             0  \n",
    " 524      1              0             0  \n",
    " 525      0              0             0  \n",
    " 526      0              0             0  \n",
    " 527      1              0             0  \n",
    " 528      0              0             0  \n",
    " 529      0              0             0  \n",
    " 530      0              0             0  \n",
    " 531      0              0             0  \n",
    " 532      1              0             0  \n",
    " 533      0              1             0  \n",
    " \n",
    " [534 rows x 10 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7fb620192358>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7fb611baebe0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'predictors': array([[ 0,  8, 21, ...,  0,  1,  0],\n",
    "        [ 0,  9, 42, ...,  0,  1,  0],\n",
    "        [ 0, 12,  1, ...,  0,  1,  0],\n",
    "        ..., \n",
    "        [ 1, 17, 25, ...,  0,  0,  0],\n",
    "        [ 1, 12, 13, ...,  1,  0,  0],\n",
    "        [ 0, 16, 33, ...,  0,  1,  0]]),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7fb620192358>,\n",
    " 'target': array([  5.1 ,   4.95,   6.67,   4.  ,   7.5 ,  13.07,   4.45,  19.47,\n",
    "         13.28,   8.75,  11.35,  11.5 ,   6.5 ,   6.25,  19.98,   7.3 ,\n",
    "          8.  ,  22.2 ,   3.65,  20.55,   5.71,   7.  ,   3.75,   4.5 ,\n",
    "          9.56,   5.75,   9.36,   6.5 ,   3.35,   4.75,   8.9 ,   4.  ,\n",
    "          4.7 ,   5.  ,   9.25,  10.67,   7.61,  10.  ,   7.5 ,  12.2 ,\n",
    "          3.35,  11.  ,  12.  ,   4.85,   4.3 ,   6.  ,  15.  ,   4.85,\n",
    "          9.  ,   6.36,   9.15,  11.  ,   4.5 ,   4.8 ,   4.  ,   5.5 ,\n",
    "          8.4 ,   6.75,  10.  ,   5.  ,   6.5 ,  10.75,   7.  ,  11.43,\n",
    "          4.  ,   9.  ,  13.  ,  12.22,   6.28,   6.75,   3.35,  16.  ,\n",
    "          5.25,   3.5 ,   4.22,   3.  ,   4.  ,  10.  ,   5.  ,  16.  ,\n",
    "         13.98,  13.26,   6.1 ,   3.75,   9.  ,   9.45,   5.5 ,   8.93,\n",
    "          6.25,   9.75,   6.73,   7.78,   2.85,   3.35,  19.98,   8.5 ,\n",
    "          9.75,  15.  ,   8.  ,  11.25,  14.  ,  10.  ,   6.5 ,   9.83,\n",
    "         18.5 ,  12.5 ,  26.  ,  14.  ,  10.5 ,  11.  ,  12.47,  12.5 ,\n",
    "         15.  ,   6.  ,   9.5 ,   5.  ,   3.75,  12.57,   6.88,   5.5 ,\n",
    "          7.  ,   4.5 ,   6.5 ,  12.  ,   5.  ,   6.5 ,   6.8 ,   8.75,\n",
    "          3.75,   4.5 ,   6.  ,   5.5 ,  13.  ,   5.65,   4.8 ,   7.  ,\n",
    "          5.25,   3.35,   8.5 ,   6.  ,   6.75,   8.89,  14.21,  10.78,\n",
    "          8.9 ,   7.5 ,   4.5 ,  11.25,  13.45,   6.  ,   4.62,  10.58,\n",
    "          5.  ,   8.2 ,   6.25,   8.5 ,  24.98,  16.65,   6.25,   4.55,\n",
    "         11.25,  21.25,  12.65,   7.5 ,  10.25,   3.35,  13.45,   4.84,\n",
    "         26.29,   6.58,  44.5 ,  15.  ,  11.25,   7.  ,  10.  ,  14.53,\n",
    "         20.  ,  22.5 ,   3.64,  10.62,  24.98,   6.  ,  19.  ,  13.2 ,\n",
    "         22.5 ,  15.  ,   6.88,  11.84,  16.14,  13.95,  13.16,   5.3 ,\n",
    "          4.5 ,  10.  ,  10.  ,  10.  ,   9.37,   5.8 ,  17.86,   1.  ,\n",
    "          8.8 ,   9.  ,  18.16,   7.81,  10.62,   4.5 ,  17.25,  10.5 ,\n",
    "          9.22,  15.  ,  22.5 ,   4.55,   9.  ,  13.33,  15.  ,   7.5 ,\n",
    "          4.25,  12.5 ,   5.13,   3.35,  11.11,   3.84,   6.4 ,   5.56,\n",
    "         10.  ,   5.65,  11.5 ,   3.5 ,   3.35,   4.75,  19.98,   3.5 ,\n",
    "          4.  ,   7.  ,   6.25,   4.5 ,  14.29,   5.  ,  13.75,  13.71,\n",
    "          7.5 ,   3.8 ,   5.  ,   9.42,   5.5 ,   3.75,   3.5 ,   5.8 ,\n",
    "         12.  ,   5.  ,   8.75,  10.  ,   8.5 ,   8.63,   9.  ,   5.5 ,\n",
    "         11.11,  10.  ,   5.2 ,   8.  ,   3.56,   5.2 ,  11.67,  11.32,\n",
    "          7.5 ,   5.5 ,   5.  ,   7.75,   5.25,   9.  ,   9.65,   5.21,\n",
    "          7.  ,  12.16,   5.25,  10.32,   3.35,   7.7 ,   9.17,   8.43,\n",
    "          4.  ,   4.13,   3.  ,   4.25,   7.53,  10.53,   5.  ,  15.03,\n",
    "         11.25,   6.25,   3.5 ,   6.85,  12.5 ,  12.  ,   6.  ,   9.5 ,\n",
    "          4.1 ,  10.43,   5.  ,   7.69,   5.5 ,   6.4 ,  12.5 ,   6.25,\n",
    "          8.  ,   9.6 ,   9.1 ,   7.5 ,   5.  ,   7.  ,   3.55,   8.5 ,\n",
    "          4.5 ,   7.88,   5.25,   5.  ,   9.33,  10.5 ,   7.5 ,   9.5 ,\n",
    "          9.6 ,   5.87,  11.02,   5.  ,   5.62,  12.5 ,  10.81,   5.4 ,\n",
    "          7.  ,   4.59,   6.  ,  11.71,   5.62,   5.5 ,   4.85,   6.75,\n",
    "          4.25,   5.75,   3.5 ,   3.35,  10.62,   8.  ,   4.75,   8.5 ,\n",
    "          8.85,   8.  ,   6.  ,   7.14,   3.4 ,   6.  ,   3.75,   8.89,\n",
    "          4.35,  13.1 ,   4.35,   3.5 ,   3.8 ,   5.26,   3.35,  16.26,\n",
    "          4.25,   4.5 ,   8.  ,   4.  ,   7.96,   4.  ,   4.15,   5.95,\n",
    "          3.6 ,   8.75,   3.4 ,   4.28,   5.35,   5.  ,   7.65,   6.94,\n",
    "          7.5 ,   3.6 ,   1.75,   3.45,   9.63,   8.49,   8.99,   3.65,\n",
    "          3.5 ,   3.43,   5.5 ,   6.93,   3.51,   3.75,   4.17,   9.57,\n",
    "         14.67,  12.5 ,   5.5 ,   5.15,   8.  ,   5.83,   3.35,   7.  ,\n",
    "         10.  ,   8.  ,   6.88,   5.55,   7.5 ,   8.93,   9.  ,   3.5 ,\n",
    "          5.77,  25.  ,   6.85,   6.5 ,   3.75,   3.5 ,   4.5 ,   2.01,\n",
    "          4.17,  13.  ,   3.98,   7.5 ,  13.12,   4.  ,   3.95,  13.  ,\n",
    "          9.  ,   4.55,   9.5 ,   4.5 ,   8.75,  10.  ,  18.  ,  24.98,\n",
    "         12.05,  22.  ,   8.75,  22.2 ,  17.25,   6.  ,   8.06,   9.24,\n",
    "         12.  ,  10.61,   5.71,  10.  ,  17.5 ,  15.  ,   7.78,   7.8 ,\n",
    "         10.  ,  24.98,  10.28,  15.  ,  12.  ,  10.58,   5.85,  11.22,\n",
    "          8.56,  13.89,   5.71,  15.79,   7.5 ,  11.25,   6.15,  13.45,\n",
    "          6.25,   6.5 ,  12.  ,   8.5 ,   8.  ,   5.75,  15.73,   9.86,\n",
    "         13.51,   5.4 ,   6.25,   5.5 ,   5.  ,   6.25,   5.75,  20.5 ,\n",
    "          5.  ,   7.  ,  18.  ,  12.  ,  20.4 ,  22.2 ,  16.42,   8.63,\n",
    "         19.38,  14.  ,  10.  ,  15.95,  20.  ,  10.  ,  24.98,  11.25,\n",
    "         22.83,  10.2 ,  10.  ,  14.  ,  12.5 ,   5.79,  24.98,   4.35,\n",
    "         11.25,   6.67,   8.  ,  18.16,  12.  ,   8.89,   9.5 ,  13.65,\n",
    "         12.  ,  15.  ,  12.67,   7.38,  15.56,   7.45,   6.25,   6.25,\n",
    "          9.37,  22.5 ,   7.5 ,   7.  ,   5.75,   7.67,  12.5 ,  16.  ,\n",
    "         11.79,  11.36,   6.1 ,  23.25,  19.88,  15.38]),\n",
    " 'to_categorical': <function keras.uti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling the model**\n",
    "\n",
    "You're now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers [here](https://keras.io/optimizers/#adam), and if you are really curious to learn more, you can read the [original paper](https://arxiv.org/abs/1412.6980v8) that introduced the Adam optimizer.\n",
    "\n",
    "In this exercise, you'll use the Adam optimizer and the mean squared error loss function. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpnvlneoxr'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'data':      wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
    " 0             5.10      0              8              21   35       1     1   \n",
    " 1             4.95      0              9              42   57       1     1   \n",
    " 2             6.67      0             12               1   19       0     0   \n",
    " 3             4.00      0             12               4   22       0     0   \n",
    " 4             7.50      0             12              17   35       0     1   \n",
    " 5            13.07      1             13               9   28       0     0   \n",
    " 6             4.45      0             10              27   43       0     0   \n",
    " 7            19.47      0             12               9   27       0     0   \n",
    " 8            13.28      0             16              11   33       0     1   \n",
    " 9             8.75      0             12               9   27       0     0   \n",
    " 10           11.35      1             12              17   35       0     1   \n",
    " 11           11.50      1             12              19   37       0     0   \n",
    " 12            6.50      0              8              27   41       0     1   \n",
    " 13            6.25      1              9              30   45       0     0   \n",
    " 14           19.98      0              9              29   44       0     1   \n",
    " 15            7.30      0             12              37   55       0     1   \n",
    " 16            8.00      0              7              44   57       0     1   \n",
    " 17           22.20      1             12              26   44       0     1   \n",
    " 18            3.65      0             11              16   33       0     0   \n",
    " 19           20.55      0             12              33   51       0     1   \n",
    " 20            5.71      1             12              16   34       1     1   \n",
    " 21            7.00      1              7              42   55       0     1   \n",
    " 22            3.75      0             12               9   27       0     0   \n",
    " 23            4.50      0             11              14   31       0     1   \n",
    " 24            9.56      0             12              23   41       0     1   \n",
    " 25            5.75      0              6              45   57       0     1   \n",
    " 26            9.36      0             12               8   26       0     1   \n",
    " 27            6.50      0             10              30   46       0     1   \n",
    " 28            3.35      0             12               8   26       1     1   \n",
    " 29            4.75      0             12               8   26       0     1   \n",
    " ..             ...    ...            ...             ...  ...     ...   ...   \n",
    " 504          11.25      0             17              10   33       1     0   \n",
    " 505           6.67      1             16              10   32       1     0   \n",
    " 506           8.00      0             16              17   39       1     1   \n",
    " 507          18.16      0             18               7   31       0     1   \n",
    " 508          12.00      0             16              14   36       1     1   \n",
    " 509           8.89      1             16              22   44       1     1   \n",
    " 510           9.50      0             17              14   37       1     1   \n",
    " 511          13.65      0             16              11   33       0     1   \n",
    " 512          12.00      1             18              23   47       0     1   \n",
    " 513          15.00      1             12              39   57       0     1   \n",
    " 514          12.67      0             16              15   37       0     1   \n",
    " 515           7.38      0             14              15   35       1     0   \n",
    " 516          15.56      0             16              10   32       0     0   \n",
    " 517           7.45      0             12              25   43       1     0   \n",
    " 518           6.25      0             14              12   32       1     1   \n",
    " 519           6.25      0             16               7   29       1     1   \n",
    " 520           9.37      1             17               7   30       0     1   \n",
    " 521          22.50      0             16              17   39       0     1   \n",
    " 522           7.50      1             16              10   32       0     1   \n",
    " 523           7.00      0             17               2   25       0     1   \n",
    " 524           5.75      1              9              34   49       1     1   \n",
    " 525           7.67      0             15              11   32       1     1   \n",
    " 526          12.50      0             15              10   31       0     0   \n",
    " 527          16.00      0             12              12   30       0     1   \n",
    " 528          11.79      1             16               6   28       1     0   \n",
    " 529          11.36      0             18               5   29       0     0   \n",
    " 530           6.10      0             12              33   51       1     1   \n",
    " 531          23.25      1             17              25   48       1     1   \n",
    " 532          19.88      1             12              13   31       0     1   \n",
    " 533          15.38      0             16              33   55       0     1   \n",
    " \n",
    "      south  manufacturing  construction  \n",
    " 0        0              1             0  \n",
    " 1        0              1             0  \n",
    " 2        0              1             0  \n",
    " 3        0              0             0  \n",
    " 4        0              0             0  \n",
    " 5        0              0             0  \n",
    " 6        1              0             0  \n",
    " 7        0              0             0  \n",
    " 8        0              1             0  \n",
    " 9        0              0             0  \n",
    " 10       0              0             0  \n",
    " 11       0              1             0  \n",
    " 12       1              0             0  \n",
    " 13       1              0             0  \n",
    " 14       1              0             0  \n",
    " 15       0              0             1  \n",
    " 16       1              0             0  \n",
    " 17       0              1             0  \n",
    " 18       0              0             0  \n",
    " 19       0              0             0  \n",
    " 20       0              1             0  \n",
    " 21       0              1             0  \n",
    " 22       0              0             0  \n",
    " 23       1              0             0  \n",
    " 24       0              0             0  \n",
    " 25       1              1             0  \n",
    " 26       0              1             0  \n",
    " 27       0              0             0  \n",
    " 28       0              1             0  \n",
    " 29       0              0             0  \n",
    " ..     ...            ...           ...  \n",
    " 504      0              0             0  \n",
    " 505      0              0             0  \n",
    " 506      0              0             0  \n",
    " 507      0              0             0  \n",
    " 508      0              0             0  \n",
    " 509      0              0             0  \n",
    " 510      0              0             0  \n",
    " 511      0              0             0  \n",
    " 512      0              0             0  \n",
    " 513      0              0             0  \n",
    " 514      0              0             0  \n",
    " 515      0              0             0  \n",
    " 516      0              0             0  \n",
    " 517      1              0             0  \n",
    " 518      0              0             0  \n",
    " 519      1              0             0  \n",
    " 520      0              0             0  \n",
    " 521      0              1             0  \n",
    " 522      0              0             0  \n",
    " 523      1              0             0  \n",
    " 524      1              0             0  \n",
    " 525      0              0             0  \n",
    " 526      0              0             0  \n",
    " 527      1              0             0  \n",
    " 528      0              0             0  \n",
    " 529      0              0             0  \n",
    " 530      0              0             0  \n",
    " 531      0              0             0  \n",
    " 532      1              0             0  \n",
    " 533      0              1             0  \n",
    " \n",
    " [534 rows x 10 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55b797cac8>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'predictors': array([[ 0,  8, 21, ...,  0,  1,  0],\n",
    "        [ 0,  9, 42, ...,  0,  1,  0],\n",
    "        [ 0, 12,  1, ...,  0,  1,  0],\n",
    "        ..., \n",
    "        [ 1, 17, 25, ...,  0,  0,  0],\n",
    "        [ 1, 12, 13, ...,  1,  0,  0],\n",
    "        [ 0, 16, 33, ...,  0,  1,  0]]),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55b797cac8>,\n",
    " 'target': array([  5.1 ,   4.95,   6.67,   4.  ,   7.5 ,  13.07,   4.45,  19.47,\n",
    "         13.28,   8.75,  11.35,  11.5 ,   6.5 ,   6.25,  19.98,   7.3 ,\n",
    "          8.  ,  22.2 ,   3.65,  20.55,   5.71,   7.  ,   3.75,   4.5 ,\n",
    "          9.56,   5.75,   9.36,   6.5 ,   3.35,   4.75,   8.9 ,   4.  ,\n",
    "          4.7 ,   5.  ,   9.25,  10.67,   7.61,  10.  ,   7.5 ,  12.2 ,\n",
    "          3.35,  11.  ,  12.  ,   4.85,   4.3 ,   6.  ,  15.  ,   4.85,\n",
    "          9.  ,   6.36,   9.15,  11.  ,   4.5 ,   4.8 ,   4.  ,   5.5 ,\n",
    "          8.4 ,   6.75,  10.  ,   5.  ,   6.5 ,  10.75,   7.  ,  11.43,\n",
    "          4.  ,   9.  ,  13.  ,  12.22,   6.28,   6.75,   3.35,  16.  ,\n",
    "          5.25,   3.5 ,   4.22,   3.  ,   4.  ,  10.  ,   5.  ,  16.  ,\n",
    "         13.98,  13.26,   6.1 ,   3.75,   9.  ,   9.45,   5.5 ,   8.93,\n",
    "          6.25,   9.75,   6.73,   7.78,   2.85,   3.35,  19.98,   8.5 ,\n",
    "          9.75,  15.  ,   8.  ,  11.25,  14.  ,  10.  ,   6.5 ,   9.83,\n",
    "         18.5 ,  12.5 ,  26.  ,  14.  ,  10.5 ,  11.  ,  12.47,  12.5 ,\n",
    "         15.  ,   6.  ,   9.5 ,   5.  ,   3.75,  12.57,   6.88,   5.5 ,\n",
    "          7.  ,   4.5 ,   6.5 ,  12.  ,   5.  ,   6.5 ,   6.8 ,   8.75,\n",
    "          3.75,   4.5 ,   6.  ,   5.5 ,  13.  ,   5.65,   4.8 ,   7.  ,\n",
    "          5.25,   3.35,   8.5 ,   6.  ,   6.75,   8.89,  14.21,  10.78,\n",
    "          8.9 ,   7.5 ,   4.5 ,  11.25,  13.45,   6.  ,   4.62,  10.58,\n",
    "          5.  ,   8.2 ,   6.25,   8.5 ,  24.98,  16.65,   6.25,   4.55,\n",
    "         11.25,  21.25,  12.65,   7.5 ,  10.25,   3.35,  13.45,   4.84,\n",
    "         26.29,   6.58,  44.5 ,  15.  ,  11.25,   7.  ,  10.  ,  14.53,\n",
    "         20.  ,  22.5 ,   3.64,  10.62,  24.98,   6.  ,  19.  ,  13.2 ,\n",
    "         22.5 ,  15.  ,   6.88,  11.84,  16.14,  13.95,  13.16,   5.3 ,\n",
    "          4.5 ,  10.  ,  10.  ,  10.  ,   9.37,   5.8 ,  17.86,   1.  ,\n",
    "          8.8 ,   9.  ,  18.16,   7.81,  10.62,   4.5 ,  17.25,  10.5 ,\n",
    "          9.22,  15.  ,  22.5 ,   4.55,   9.  ,  13.33,  15.  ,   7.5 ,\n",
    "          4.25,  12.5 ,   5.13,   3.35,  11.11,   3.84,   6.4 ,   5.56,\n",
    "         10.  ,   5.65,  11.5 ,   3.5 ,   3.35,   4.75,  19.98,   3.5 ,\n",
    "          4.  ,   7.  ,   6.25,   4.5 ,  14.29,   5.  ,  13.75,  13.71,\n",
    "          7.5 ,   3.8 ,   5.  ,   9.42,   5.5 ,   3.75,   3.5 ,   5.8 ,\n",
    "         12.  ,   5.  ,   8.75,  10.  ,   8.5 ,   8.63,   9.  ,   5.5 ,\n",
    "         11.11,  10.  ,   5.2 ,   8.  ,   3.56,   5.2 ,  11.67,  11.32,\n",
    "          7.5 ,   5.5 ,   5.  ,   7.75,   5.25,   9.  ,   9.65,   5.21,\n",
    "          7.  ,  12.16,   5.25,  10.32,   3.35,   7.7 ,   9.17,   8.43,\n",
    "          4.  ,   4.13,   3.  ,   4.25,   7.53,  10.53,   5.  ,  15.03,\n",
    "         11.25,   6.25,   3.5 ,   6.85,  12.5 ,  12.  ,   6.  ,   9.5 ,\n",
    "          4.1 ,  10.43,   5.  ,   7.69,   5.5 ,   6.4 ,  12.5 ,   6.25,\n",
    "          8.  ,   9.6 ,   9.1 ,   7.5 ,   5.  ,   7.  ,   3.55,   8.5 ,\n",
    "          4.5 ,   7.88,   5.25,   5.  ,   9.33,  10.5 ,   7.5 ,   9.5 ,\n",
    "          9.6 ,   5.87,  11.02,   5.  ,   5.62,  12.5 ,  10.81,   5.4 ,\n",
    "          7.  ,   4.59,   6.  ,  11.71,   5.62,   5.5 ,   4.85,   6.75,\n",
    "          4.25,   5.75,   3.5 ,   3.35,  10.62,   8.  ,   4.75,   8.5 ,\n",
    "          8.85,   8.  ,   6.  ,   7.14,   3.4 ,   6.  ,   3.75,   8.89,\n",
    "          4.35,  13.1 ,   4.35,   3.5 ,   3.8 ,   5.26,   3.35,  16.26,\n",
    "          4.25,   4.5 ,   8.  ,   4.  ,   7.96,   4.  ,   4.15,   5.95,\n",
    "          3.6 ,   8.75,   3.4 ,   4.28,   5.35,   5.  ,   7.65,   6.94,\n",
    "          7.5 ,   3.6 ,   1.75,   3.45,   9.63,   8.49,   8.99,   3.65,\n",
    "          3.5 ,   3.43,   5.5 ,   6.93,   3.51,   3.75,   4.17,   9.57,\n",
    "         14.67,  12.5 ,   5.5 ,   5.15,   8.  ,   5.83,   3.35,   7.  ,\n",
    "         10.  ,   8.  ,   6.88,   5.55,   7.5 ,   8.93,   9.  ,   3.5 ,\n",
    "          5.77,  25.  ,   6.85,   6.5 ,   3.75,   3.5 ,   4.5 ,   2.01,\n",
    "          4.17,  13.  ,   3.98,   7.5 ,  13.12,   4.  ,   3.95,  13.  ,\n",
    "          9.  ,   4.55,   9.5 ,   4.5 ,   8.75,  10.  ,  18.  ,  24.98,\n",
    "         12.05,  22.  ,   8.75,  22.2 ,  17.25,   6.  ,   8.06,   9.24,\n",
    "         12.  ,  10.61,   5.71,  10.  ,  17.5 ,  15.  ,   7.78,   7.8 ,\n",
    "         10.  ,  24.98,  10.28,  15.  ,  12.  ,  10.58,   5.85,  11.22,\n",
    "          8.56,  13.89,   5.71,  15.79,   7.5 ,  11.25,   6.15,  13.45,\n",
    "          6.25,   6.5 ,  12.  ,   8.5 ,   8.  ,   5.75,  15.73,   9.86,\n",
    "         13.51,   5.4 ,   6.25,   5.5 ,   5.  ,   6.25,   5.75,  20.5 ,\n",
    "          5.  ,   7.  ,  18.  ,  12.  ,  20.4 ,  22.2 ,  16.42,   8.63,\n",
    "         19.38,  14.  ,  10.  ,  15.95,  20.  ,  10.  ,  24.98,  11.25,\n",
    "         22.83,  10.2 ,  10.  ,  14.  ,  12.5 ,   5.79,  24.98,   4.35,\n",
    "         11.25,   6.67,   8.  ,  18.16,  12.  ,   8.89,   9.5 ,  13.65,\n",
    "         12.  ,  15.  ,  12.67,   7.38,  15.56,   7.45,   6.25,   6.25,\n",
    "          9.37,  22.5 ,   7.5 ,   7.  ,   5.75,   7.67,  12.5 ,  16.  ,\n",
    "         11.79,  11.36,   6.1 ,  23.25,  19.88,  15.38]),\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model**\n",
    "\n",
    "You're at the most fun part. You'll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called `predictors` and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmp2kydnlh4'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'data':      wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
    " 0             5.10      0              8              21   35       1     1   \n",
    " 1             4.95      0              9              42   57       1     1   \n",
    " 2             6.67      0             12               1   19       0     0   \n",
    " 3             4.00      0             12               4   22       0     0   \n",
    " 4             7.50      0             12              17   35       0     1   \n",
    " 5            13.07      1             13               9   28       0     0   \n",
    " 6             4.45      0             10              27   43       0     0   \n",
    " 7            19.47      0             12               9   27       0     0   \n",
    " 8            13.28      0             16              11   33       0     1   \n",
    " 9             8.75      0             12               9   27       0     0   \n",
    " 10           11.35      1             12              17   35       0     1   \n",
    " 11           11.50      1             12              19   37       0     0   \n",
    " 12            6.50      0              8              27   41       0     1   \n",
    " 13            6.25      1              9              30   45       0     0   \n",
    " 14           19.98      0              9              29   44       0     1   \n",
    " 15            7.30      0             12              37   55       0     1   \n",
    " 16            8.00      0              7              44   57       0     1   \n",
    " 17           22.20      1             12              26   44       0     1   \n",
    " 18            3.65      0             11              16   33       0     0   \n",
    " 19           20.55      0             12              33   51       0     1   \n",
    " 20            5.71      1             12              16   34       1     1   \n",
    " 21            7.00      1              7              42   55       0     1   \n",
    " 22            3.75      0             12               9   27       0     0   \n",
    " 23            4.50      0             11              14   31       0     1   \n",
    " 24            9.56      0             12              23   41       0     1   \n",
    " 25            5.75      0              6              45   57       0     1   \n",
    " 26            9.36      0             12               8   26       0     1   \n",
    " 27            6.50      0             10              30   46       0     1   \n",
    " 28            3.35      0             12               8   26       1     1   \n",
    " 29            4.75      0             12               8   26       0     1   \n",
    " ..             ...    ...            ...             ...  ...     ...   ...   \n",
    " 504          11.25      0             17              10   33       1     0   \n",
    " 505           6.67      1             16              10   32       1     0   \n",
    " 506           8.00      0             16              17   39       1     1   \n",
    " 507          18.16      0             18               7   31       0     1   \n",
    " 508          12.00      0             16              14   36       1     1   \n",
    " 509           8.89      1             16              22   44       1     1   \n",
    " 510           9.50      0             17              14   37       1     1   \n",
    " 511          13.65      0             16              11   33       0     1   \n",
    " 512          12.00      1             18              23   47       0     1   \n",
    " 513          15.00      1             12              39   57       0     1   \n",
    " 514          12.67      0             16              15   37       0     1   \n",
    " 515           7.38      0             14              15   35       1     0   \n",
    " 516          15.56      0             16              10   32       0     0   \n",
    " 517           7.45      0             12              25   43       1     0   \n",
    " 518           6.25      0             14              12   32       1     1   \n",
    " 519           6.25      0             16               7   29       1     1   \n",
    " 520           9.37      1             17               7   30       0     1   \n",
    " 521          22.50      0             16              17   39       0     1   \n",
    " 522           7.50      1             16              10   32       0     1   \n",
    " 523           7.00      0             17               2   25       0     1   \n",
    " 524           5.75      1              9              34   49       1     1   \n",
    " 525           7.67      0             15              11   32       1     1   \n",
    " 526          12.50      0             15              10   31       0     0   \n",
    " 527          16.00      0             12              12   30       0     1   \n",
    " 528          11.79      1             16               6   28       1     0   \n",
    " 529          11.36      0             18               5   29       0     0   \n",
    " 530           6.10      0             12              33   51       1     1   \n",
    " 531          23.25      1             17              25   48       1     1   \n",
    " 532          19.88      1             12              13   31       0     1   \n",
    " 533          15.38      0             16              33   55       0     1   \n",
    " \n",
    "      south  manufacturing  construction  \n",
    " 0        0              1             0  \n",
    " 1        0              1             0  \n",
    " 2        0              1             0  \n",
    " 3        0              0             0  \n",
    " 4        0              0             0  \n",
    " 5        0              0             0  \n",
    " 6        1              0             0  \n",
    " 7        0              0             0  \n",
    " 8        0              1             0  \n",
    " 9        0              0             0  \n",
    " 10       0              0             0  \n",
    " 11       0              1             0  \n",
    " 12       1              0             0  \n",
    " 13       1              0             0  \n",
    " 14       1              0             0  \n",
    " 15       0              0             1  \n",
    " 16       1              0             0  \n",
    " 17       0              1             0  \n",
    " 18       0              0             0  \n",
    " 19       0              0             0  \n",
    " 20       0              1             0  \n",
    " 21       0              1             0  \n",
    " 22       0              0             0  \n",
    " 23       1              0             0  \n",
    " 24       0              0             0  \n",
    " 25       1              1             0  \n",
    " 26       0              1             0  \n",
    " 27       0              0             0  \n",
    " 28       0              1             0  \n",
    " 29       0              0             0  \n",
    " ..     ...            ...           ...  \n",
    " 504      0              0             0  \n",
    " 505      0              0             0  \n",
    " 506      0              0             0  \n",
    " 507      0              0             0  \n",
    " 508      0              0             0  \n",
    " 509      0              0             0  \n",
    " 510      0              0             0  \n",
    " 511      0              0             0  \n",
    " 512      0              0             0  \n",
    " 513      0              0             0  \n",
    " 514      0              0             0  \n",
    " 515      0              0             0  \n",
    " 516      0              0             0  \n",
    " 517      1              0             0  \n",
    " 518      0              0             0  \n",
    " 519      1              0             0  \n",
    " 520      0              0             0  \n",
    " 521      0              1             0  \n",
    " 522      0              0             0  \n",
    " 523      1              0             0  \n",
    " 524      1              0             0  \n",
    " 525      0              0             0  \n",
    " 526      0              0             0  \n",
    " 527      1              0             0  \n",
    " 528      0              0             0  \n",
    " 529      0              0             0  \n",
    " 530      0              0             0  \n",
    " 531      0              0             0  \n",
    " 532      1              0             0  \n",
    " 533      0              1             0  \n",
    " \n",
    " [534 rows x 10 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55b614acf8>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'predictors': array([[ 0,  8, 21, ...,  0,  1,  0],\n",
    "        [ 0,  9, 42, ...,  0,  1,  0],\n",
    "        [ 0, 12,  1, ...,  0,  1,  0],\n",
    "        ..., \n",
    "        [ 1, 17, 25, ...,  0,  0,  0],\n",
    "        [ 1, 12, 13, ...,  1,  0,  0],\n",
    "        [ 0, 16, 33, ...,  0,  1,  0]]),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55b614acf8>,\n",
    " 'target': array([  5.1 ,   4.95,   6.67,   4.  ,   7.5 ,  13.07,   4.45,  19.47,\n",
    "         13.28,   8.75,  11.35,  11.5 ,   6.5 ,   6.25,  19.98,   7.3 ,\n",
    "          8.  ,  22.2 ,   3.65,  20.55,   5.71,   7.  ,   3.75,   4.5 ,\n",
    "          9.56,   5.75,   9.36,   6.5 ,   3.35,   4.75,   8.9 ,   4.  ,\n",
    "          4.7 ,   5.  ,   9.25,  10.67,   7.61,  10.  ,   7.5 ,  12.2 ,\n",
    "          3.35,  11.  ,  12.  ,   4.85,   4.3 ,   6.  ,  15.  ,   4.85,\n",
    "          9.  ,   6.36,   9.15,  11.  ,   4.5 ,   4.8 ,   4.  ,   5.5 ,\n",
    "          8.4 ,   6.75,  10.  ,   5.  ,   6.5 ,  10.75,   7.  ,  11.43,\n",
    "          4.  ,   9.  ,  13.  ,  12.22,   6.28,   6.75,   3.35,  16.  ,\n",
    "          5.25,   3.5 ,   4.22,   3.  ,   4.  ,  10.  ,   5.  ,  16.  ,\n",
    "         13.98,  13.26,   6.1 ,   3.75,   9.  ,   9.45,   5.5 ,   8.93,\n",
    "          6.25,   9.75,   6.73,   7.78,   2.85,   3.35,  19.98,   8.5 ,\n",
    "          9.75,  15.  ,   8.  ,  11.25,  14.  ,  10.  ,   6.5 ,   9.83,\n",
    "         18.5 ,  12.5 ,  26.  ,  14.  ,  10.5 ,  11.  ,  12.47,  12.5 ,\n",
    "         15.  ,   6.  ,   9.5 ,   5.  ,   3.75,  12.57,   6.88,   5.5 ,\n",
    "          7.  ,   4.5 ,   6.5 ,  12.  ,   5.  ,   6.5 ,   6.8 ,   8.75,\n",
    "          3.75,   4.5 ,   6.  ,   5.5 ,  13.  ,   5.65,   4.8 ,   7.  ,\n",
    "          5.25,   3.35,   8.5 ,   6.  ,   6.75,   8.89,  14.21,  10.78,\n",
    "          8.9 ,   7.5 ,   4.5 ,  11.25,  13.45,   6.  ,   4.62,  10.58,\n",
    "          5.  ,   8.2 ,   6.25,   8.5 ,  24.98,  16.65,   6.25,   4.55,\n",
    "         11.25,  21.25,  12.65,   7.5 ,  10.25,   3.35,  13.45,   4.84,\n",
    "         26.29,   6.58,  44.5 ,  15.  ,  11.25,   7.  ,  10.  ,  14.53,\n",
    "         20.  ,  22.5 ,   3.64,  10.62,  24.98,   6.  ,  19.  ,  13.2 ,\n",
    "         22.5 ,  15.  ,   6.88,  11.84,  16.14,  13.95,  13.16,   5.3 ,\n",
    "          4.5 ,  10.  ,  10.  ,  10.  ,   9.37,   5.8 ,  17.86,   1.  ,\n",
    "          8.8 ,   9.  ,  18.16,   7.81,  10.62,   4.5 ,  17.25,  10.5 ,\n",
    "          9.22,  15.  ,  22.5 ,   4.55,   9.  ,  13.33,  15.  ,   7.5 ,\n",
    "          4.25,  12.5 ,   5.13,   3.35,  11.11,   3.84,   6.4 ,   5.56,\n",
    "         10.  ,   5.65,  11.5 ,   3.5 ,   3.35,   4.75,  19.98,   3.5 ,\n",
    "          4.  ,   7.  ,   6.25,   4.5 ,  14.29,   5.  ,  13.75,  13.71,\n",
    "          7.5 ,   3.8 ,   5.  ,   9.42,   5.5 ,   3.75,   3.5 ,   5.8 ,\n",
    "         12.  ,   5.  ,   8.75,  10.  ,   8.5 ,   8.63,   9.  ,   5.5 ,\n",
    "         11.11,  10.  ,   5.2 ,   8.  ,   3.56,   5.2 ,  11.67,  11.32,\n",
    "          7.5 ,   5.5 ,   5.  ,   7.75,   5.25,   9.  ,   9.65,   5.21,\n",
    "          7.  ,  12.16,   5.25,  10.32,   3.35,   7.7 ,   9.17,   8.43,\n",
    "          4.  ,   4.13,   3.  ,   4.25,   7.53,  10.53,   5.  ,  15.03,\n",
    "         11.25,   6.25,   3.5 ,   6.85,  12.5 ,  12.  ,   6.  ,   9.5 ,\n",
    "          4.1 ,  10.43,   5.  ,   7.69,   5.5 ,   6.4 ,  12.5 ,   6.25,\n",
    "          8.  ,   9.6 ,   9.1 ,   7.5 ,   5.  ,   7.  ,   3.55,   8.5 ,\n",
    "          4.5 ,   7.88,   5.25,   5.  ,   9.33,  10.5 ,   7.5 ,   9.5 ,\n",
    "          9.6 ,   5.87,  11.02,   5.  ,   5.62,  12.5 ,  10.81,   5.4 ,\n",
    "          7.  ,   4.59,   6.  ,  11.71,   5.62,   5.5 ,   4.85,   6.75,\n",
    "          4.25,   5.75,   3.5 ,   3.35,  10.62,   8.  ,   4.75,   8.5 ,\n",
    "          8.85,   8.  ,   6.  ,   7.14,   3.4 ,   6.  ,   3.75,   8.89,\n",
    "          4.35,  13.1 ,   4.35,   3.5 ,   3.8 ,   5.26,   3.35,  16.26,\n",
    "          4.25,   4.5 ,   8.  ,   4.  ,   7.96,   4.  ,   4.15,   5.95,\n",
    "          3.6 ,   8.75,   3.4 ,   4.28,   5.35,   5.  ,   7.65,   6.94,\n",
    "          7.5 ,   3.6 ,   1.75,   3.45,   9.63,   8.49,   8.99,   3.65,\n",
    "          3.5 ,   3.43,   5.5 ,   6.93,   3.51,   3.75,   4.17,   9.57,\n",
    "         14.67,  12.5 ,   5.5 ,   5.15,   8.  ,   5.83,   3.35,   7.  ,\n",
    "         10.  ,   8.  ,   6.88,   5.55,   7.5 ,   8.93,   9.  ,   3.5 ,\n",
    "          5.77,  25.  ,   6.85,   6.5 ,   3.75,   3.5 ,   4.5 ,   2.01,\n",
    "          4.17,  13.  ,   3.98,   7.5 ,  13.12,   4.  ,   3.95,  13.  ,\n",
    "          9.  ,   4.55,   9.5 ,   4.5 ,   8.75,  10.  ,  18.  ,  24.98,\n",
    "         12.05,  22.  ,   8.75,  22.2 ,  17.25,   6.  ,   8.06,   9.24,\n",
    "         12.  ,  10.61,   5.71,  10.  ,  17.5 ,  15.  ,   7.78,   7.8 ,\n",
    "         10.  ,  24.98,  10.28,  15.  ,  12.  ,  10.58,   5.85,  11.22,\n",
    "          8.56,  13.89,   5.71,  15.79,   7.5 ,  11.25,   6.15,  13.45,\n",
    "          6.25,   6.5 ,  12.  ,   8.5 ,   8.  ,   5.75,  15.73,   9.86,\n",
    "         13.51,   5.4 ,   6.25,   5.5 ,   5.  ,   6.25,   5.75,  20.5 ,\n",
    "          5.  ,   7.  ,  18.  ,  12.  ,  20.4 ,  22.2 ,  16.42,   8.63,\n",
    "         19.38,  14.  ,  10.  ,  15.95,  20.  ,  10.  ,  24.98,  11.25,\n",
    "         22.83,  10.2 ,  10.  ,  14.  ,  12.5 ,   5.79,  24.98,   4.35,\n",
    "         11.25,   6.67,   8.  ,  18.16,  12.  ,   8.89,   9.5 ,  13.65,\n",
    "         12.  ,  15.  ,  12.67,   7.38,  15.56,   7.45,   6.25,   6.25,\n",
    "          9.37,  22.5 ,   7.5 ,   7.  ,   5.75,   7.67,  12.5 ,  16.  ,\n",
    "         11.79,  11.36,   6.1 ,  23.25,  19.88,  15.38]),\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 85.5178\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 45.9071     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 74.4328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 28.9703     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 14.2559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 23.4206     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 12.4489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.4502     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 9.1276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.4620    \n",
    "Epoch 6/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 20.8471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.3464     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 13.4813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.1690     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 10.7564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.1876     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 36.3705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 21.6074     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/534 [>.............................] - ETA: 0s - loss: 19.8555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "534/534 [==============================] - 0s - loss: 20.9601\n",
    "Out[1]: <keras.callbacks.History at 0x7f55b44cd5c0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding your classification data**\n",
    "\n",
    "Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as `age`, `fare` and where each passenger embarked from to predict who will survive. This data is from a [tutorial on data science competitions](https://www.kaggle.com/c/titanic). Look here for descriptions of the features.\n",
    "\n",
    "The data is pre-loaded in a pandas DataFrame called `df`.\n",
    "\n",
    "It's smart to review the maximum and minimum values of each variable to ensure the data isn't misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the `.describe()` method in the IPython Shell to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In [1]: df.describe()\n",
    "Out[1]: \n",
    "         survived      pclass         age       sibsp       parch        fare  \\\n",
    "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
    "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208   \n",
    "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429   \n",
    "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000   \n",
    "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400   \n",
    "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200   \n",
    "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000   \n",
    "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200   \n",
    "\n",
    "             male  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
    "count  891.000000               891.000000                891.000000   \n",
    "mean     0.647587                 0.188552                  0.086420   \n",
    "std      0.477990                 0.391372                  0.281141   \n",
    "min      0.000000                 0.000000                  0.000000   \n",
    "25%      0.000000                 0.000000                  0.000000   \n",
    "50%      1.000000                 0.000000                  0.000000   \n",
    "75%      1.000000                 0.000000                  0.000000   \n",
    "max      1.000000                 1.000000                  1.000000   \n",
    "\n",
    "       embarked_from_southampton  \n",
    "count                 891.000000  \n",
    "mean                    0.722783  \n",
    "std                     0.447876  \n",
    "min                     0.000000  \n",
    "25%                     0.000000  \n",
    "50%                     1.000000  \n",
    "75%                     1.000000  \n",
    "max                     1.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last steps in classification models**\n",
    "\n",
    "You'll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called `df`. You'll take information about the passengers and predict which ones survived.\n",
    "\n",
    "The predictive variables are stored in a NumPy array `predictors`. The target to predict is in `df.survived`, though you'll have to manipulate it for keras. The number of predictive features is stored in `n_cols`.\n",
    "\n",
    "Here, you'll use the `'sgd'` optimizer, which stands for [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). You'll learn more about this in the next chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpifkdt8fv'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'df':      survived  pclass        age  sibsp  parch      fare  male  \\\n",
    " 0           0       3  22.000000      1      0    7.2500     1   \n",
    " 1           1       1  38.000000      1      0   71.2833     0   \n",
    " 2           1       3  26.000000      0      0    7.9250     0   \n",
    " 3           1       1  35.000000      1      0   53.1000     0   \n",
    " 4           0       3  35.000000      0      0    8.0500     1   \n",
    " 5           0       3  29.699118      0      0    8.4583     1   \n",
    " 6           0       1  54.000000      0      0   51.8625     1   \n",
    " 7           0       3   2.000000      3      1   21.0750     1   \n",
    " 8           1       3  27.000000      0      2   11.1333     0   \n",
    " 9           1       2  14.000000      1      0   30.0708     0   \n",
    " 10          1       3   4.000000      1      1   16.7000     0   \n",
    " 11          1       1  58.000000      0      0   26.5500     0   \n",
    " 12          0       3  20.000000      0      0    8.0500     1   \n",
    " 13          0       3  39.000000      1      5   31.2750     1   \n",
    " 14          0       3  14.000000      0      0    7.8542     0   \n",
    " 15          1       2  55.000000      0      0   16.0000     0   \n",
    " 16          0       3   2.000000      4      1   29.1250     1   \n",
    " 17          1       2  29.699118      0      0   13.0000     1   \n",
    " 18          0       3  31.000000      1      0   18.0000     0   \n",
    " 19          1       3  29.699118      0      0    7.2250     0   \n",
    " 20          0       2  35.000000      0      0   26.0000     1   \n",
    " 21          1       2  34.000000      0      0   13.0000     1   \n",
    " 22          1       3  15.000000      0      0    8.0292     0   \n",
    " 23          1       1  28.000000      0      0   35.5000     1   \n",
    " 24          0       3   8.000000      3      1   21.0750     0   \n",
    " 25          1       3  38.000000      1      5   31.3875     0   \n",
    " 26          0       3  29.699118      0      0    7.2250     1   \n",
    " 27          0       1  19.000000      3      2  263.0000     1   \n",
    " 28          1       3  29.699118      0      0    7.8792     0   \n",
    " 29          0       3  29.699118      0      0    7.8958     1   \n",
    " ..        ...     ...        ...    ...    ...       ...   ...   \n",
    " 861         0       2  21.000000      1      0   11.5000     1   \n",
    " 862         1       1  48.000000      0      0   25.9292     0   \n",
    " 863         0       3  29.699118      8      2   69.5500     0   \n",
    " 864         0       2  24.000000      0      0   13.0000     1   \n",
    " 865         1       2  42.000000      0      0   13.0000     0   \n",
    " 866         1       2  27.000000      1      0   13.8583     0   \n",
    " 867         0       1  31.000000      0      0   50.4958     1   \n",
    " 868         0       3  29.699118      0      0    9.5000     1   \n",
    " 869         1       3   4.000000      1      1   11.1333     1   \n",
    " 870         0       3  26.000000      0      0    7.8958     1   \n",
    " 871         1       1  47.000000      1      1   52.5542     0   \n",
    " 872         0       1  33.000000      0      0    5.0000     1   \n",
    " 873         0       3  47.000000      0      0    9.0000     1   \n",
    " 874         1       2  28.000000      1      0   24.0000     0   \n",
    " 875         1       3  15.000000      0      0    7.2250     0   \n",
    " 876         0       3  20.000000      0      0    9.8458     1   \n",
    " 877         0       3  19.000000      0      0    7.8958     1   \n",
    " 878         0       3  29.699118      0      0    7.8958     1   \n",
    " 879         1       1  56.000000      0      1   83.1583     0   \n",
    " 880         1       2  25.000000      0      1   26.0000     0   \n",
    " 881         0       3  33.000000      0      0    7.8958     1   \n",
    " 882         0       3  22.000000      0      0   10.5167     0   \n",
    " 883         0       2  28.000000      0      0   10.5000     1   \n",
    " 884         0       3  25.000000      0      0    7.0500     1   \n",
    " 885         0       3  39.000000      0      5   29.1250     0   \n",
    " 886         0       2  27.000000      0      0   13.0000     1   \n",
    " 887         1       1  19.000000      0      0   30.0000     0   \n",
    " 888         0       3  29.699118      1      2   23.4500     0   \n",
    " 889         1       1  26.000000      0      0   30.0000     1   \n",
    " 890         0       3  32.000000      0      0    7.7500     1   \n",
    " \n",
    "      age_was_missing  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
    " 0                  0                        0                         0   \n",
    " 1                  0                        1                         0   \n",
    " 2                  0                        0                         0   \n",
    " 3                  0                        0                         0   \n",
    " 4                  0                        0                         0   \n",
    " 5                  1                        0                         1   \n",
    " 6                  0                        0                         0   \n",
    " 7                  0                        0                         0   \n",
    " 8                  0                        0                         0   \n",
    " 9                  0                        1                         0   \n",
    " 10                 0                        0                         0   \n",
    " 11                 0                        0                         0   \n",
    " 12                 0                        0                         0   \n",
    " 13                 0                        0                         0   \n",
    " 14                 0                        0                         0   \n",
    " 15                 0                        0                         0   \n",
    " 16                 0                        0                         1   \n",
    " 17                 1                        0                         0   \n",
    " 18                 0                        0                         0   \n",
    " 19                 1                        1                         0   \n",
    " 20                 0                        0                         0   \n",
    " 21                 0                        0                         0   \n",
    " 22                 0                        0                         1   \n",
    " 23                 0                        0                         0   \n",
    " 24                 0                        0                         0   \n",
    " 25                 0                        0                         0   \n",
    " 26                 1                        1                         0   \n",
    " 27                 0                        0                         0   \n",
    " 28                 1                        0                         1   \n",
    " 29                 1                        0                         0   \n",
    " ..               ...                      ...                       ...   \n",
    " 861                0                        0                         0   \n",
    " 862                0                        0                         0   \n",
    " 863                1                        0                         0   \n",
    " 864                0                        0                         0   \n",
    " 865                0                        0                         0   \n",
    " 866                0                        1                         0   \n",
    " 867                0                        0                         0   \n",
    " 868                1                        0                         0   \n",
    " 869                0                        0                         0   \n",
    " 870                0                        0                         0   \n",
    " 871                0                        0                         0   \n",
    " 872                0                        0                         0   \n",
    " 873                0                        0                         0   \n",
    " 874                0                        1                         0   \n",
    " 875                0                        1                         0   \n",
    " 876                0                        0                         0   \n",
    " 877                0                        0                         0   \n",
    " 878                1                        0                         0   \n",
    " 879                0                        1                         0   \n",
    " 880                0                        0                         0   \n",
    " 881                0                        0                         0   \n",
    " 882                0                        0                         0   \n",
    " 883                0                        0                         0   \n",
    " 884                0                        0                         0   \n",
    " 885                0                        0                         1   \n",
    " 886                0                        0                         0   \n",
    " 887                0                        0                         0   \n",
    " 888                1                        0                         0   \n",
    " 889                0                        1                         0   \n",
    " 890                0                        0                         1   \n",
    " \n",
    "      embarked_from_southampton  \n",
    " 0                            1  \n",
    " 1                            0  \n",
    " 2                            1  \n",
    " 3                            1  \n",
    " 4                            1  \n",
    " 5                            0  \n",
    " 6                            1  \n",
    " 7                            1  \n",
    " 8                            1  \n",
    " 9                            0  \n",
    " 10                           1  \n",
    " 11                           1  \n",
    " 12                           1  \n",
    " 13                           1  \n",
    " 14                           1  \n",
    " 15                           1  \n",
    " 16                           0  \n",
    " 17                           1  \n",
    " 18                           1  \n",
    " 19                           0  \n",
    " 20                           1  \n",
    " 21                           1  \n",
    " 22                           0  \n",
    " 23                           1  \n",
    " 24                           1  \n",
    " 25                           1  \n",
    " 26                           0  \n",
    " 27                           1  \n",
    " 28                           0  \n",
    " 29                           1  \n",
    " ..                         ...  \n",
    " 861                          1  \n",
    " 862                          1  \n",
    " 863                          1  \n",
    " 864                          1  \n",
    " 865                          1  \n",
    " 866                          0  \n",
    " 867                          1  \n",
    " 868                          1  \n",
    " 869                          1  \n",
    " 870                          1  \n",
    " 871                          1  \n",
    " 872                          1  \n",
    " 873                          1  \n",
    " 874                          0  \n",
    " 875                          0  \n",
    " 876                          1  \n",
    " 877                          1  \n",
    " 878                          1  \n",
    " 879                          0  \n",
    " 880                          1  \n",
    " 881                          1  \n",
    " 882                          1  \n",
    " 883                          1  \n",
    " 884                          1  \n",
    " 885                          0  \n",
    " 886                          1  \n",
    " 887                          1  \n",
    " 888                          1  \n",
    " 889                          0  \n",
    " 890                          0  \n",
    " \n",
    " [891 rows x 11 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55c3a36400>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'n_cols': 10,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'predictors': array([[  3.        ,  22.        ,   1.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        [  1.        ,  38.        ,   1.        , ...,   1.        ,\n",
    "           0.        ,   0.        ],\n",
    "        [  3.        ,  26.        ,   0.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        ..., \n",
    "        [  3.        ,  29.69911765,   1.        , ...,   0.        ,\n",
    "           0.        ,   1.        ],\n",
    "        [  1.        ,  26.        ,   0.        , ...,   1.        ,\n",
    "           0.        ,   0.        ],\n",
    "        [  3.        ,  32.        ,   0.        , ...,   0.        ,\n",
    "           1.        ,   0.        ]]),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55c3a36400>,\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))  \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 2.2860 - acc: 0.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 2.7017 - acc: 0.5771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 2.0223 - acc: 0.6150     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 1.3087 - acc: 0.4062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 1.1448 - acc: 0.6229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "864/891 [============================>.] - ETA: 0s - loss: 1.1202 - acc: 0.6169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 1.1082 - acc: 0.6184     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 1.2949 - acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "416/891 [=============>................] - ETA: 0s - loss: 0.9214 - acc: 0.6154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/891 [=========================>....] - ETA: 0s - loss: 0.9359 - acc: 0.6138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.9109 - acc: 0.6173     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.8078 - acc: 0.5625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "576/891 [==================>...........] - ETA: 0s - loss: 0.7991 - acc: 0.6597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7541 - acc: 0.6566     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7259 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/891 [======================>.......] - ETA: 0s - loss: 0.7659 - acc: 0.6605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7704 - acc: 0.6465     \n",
    "Epoch 6/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.5819 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "448/891 [==============>...............] - ETA: 0s - loss: 0.7569 - acc: 0.6540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.7122 - acc: 0.6532     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7170 - acc: 0.6875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "480/891 [===============>..............] - ETA: 0s - loss: 0.6419 - acc: 0.6708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6810 - acc: 0.6813     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.5142 - acc: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "576/891 [==================>...........] - ETA: 0s - loss: 0.6646 - acc: 0.6858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6461 - acc: 0.6790     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.7385 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "416/891 [=============>................] - ETA: 0s - loss: 0.6555 - acc: 0.6587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/891 [=========================>....] - ETA: 0s - loss: 0.6653 - acc: 0.6675\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6660 - acc: 0.6723     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/891 [>.............................] - ETA: 0s - loss: 0.9764 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "352/891 [==========>...................] - ETA: 0s - loss: 0.6541 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/891 [=====================>........] - ETA: 0s - loss: 0.6135 - acc: 0.6771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "891/891 [==============================] - 0s - loss: 0.6132 - acc: 0.6745\n",
    "Out[2]: <keras.callbacks.History at 0x7f55b4c83fd0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making predictions**\n",
    "\n",
    "The trained network from your previous coding exercise is now stored as `model`. New data to make predictions is stored in a NumPy array as `pred_data`. Use `model` to make predictions on your new data.\n",
    "\n",
    "In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'Dense': keras.layers.core.Dense,\n",
    " 'In': ['', 'vars()'],\n",
    " 'Out': {},\n",
    " 'Sequential': keras.models.Sequential,\n",
    " 'X': array([[3, 22.0, 1, ..., 0, 0, 1],\n",
    "        [1, 38.0, 1, ..., 1, 0, 0],\n",
    "        [3, 26.0, 0, ..., 0, 0, 1],\n",
    "        ..., \n",
    "        [3, 29.69911764705882, 1, ..., 0, 0, 1],\n",
    "        [1, 26.0, 0, ..., 1, 0, 0],\n",
    "        [3, 32.0, 0, ..., 0, 1, 0]], dtype=object),\n",
    " '_': '',\n",
    " '__': '',\n",
    " '___': '',\n",
    " '__builtin__': <module 'builtins' (built-in)>,\n",
    " '__builtins__': <module 'builtins' (built-in)>,\n",
    " '__name__': '__main__',\n",
    " '_dh': ['/tmp/tmpd9pr98uj'],\n",
    " '_i': '',\n",
    " '_i1': 'vars()',\n",
    " '_ih': ['', 'vars()'],\n",
    " '_ii': '',\n",
    " '_iii': '',\n",
    " '_oh': {},\n",
    " '_sh': <module 'IPython.core.shadowns' from '/var/lib/python/site-packages/IPython/core/shadowns.py'>,\n",
    " 'data':      survived  pclass        age  sibsp  parch      fare  male  \\\n",
    " 0           0       3  22.000000      1      0    7.2500     1   \n",
    " 1           1       1  38.000000      1      0   71.2833     0   \n",
    " 2           1       3  26.000000      0      0    7.9250     0   \n",
    " 3           1       1  35.000000      1      0   53.1000     0   \n",
    " 4           0       3  35.000000      0      0    8.0500     1   \n",
    " 5           0       3  29.699118      0      0    8.4583     1   \n",
    " 6           0       1  54.000000      0      0   51.8625     1   \n",
    " 7           0       3   2.000000      3      1   21.0750     1   \n",
    " 8           1       3  27.000000      0      2   11.1333     0   \n",
    " 9           1       2  14.000000      1      0   30.0708     0   \n",
    " 10          1       3   4.000000      1      1   16.7000     0   \n",
    " 11          1       1  58.000000      0      0   26.5500     0   \n",
    " 12          0       3  20.000000      0      0    8.0500     1   \n",
    " 13          0       3  39.000000      1      5   31.2750     1   \n",
    " 14          0       3  14.000000      0      0    7.8542     0   \n",
    " 15          1       2  55.000000      0      0   16.0000     0   \n",
    " 16          0       3   2.000000      4      1   29.1250     1   \n",
    " 17          1       2  29.699118      0      0   13.0000     1   \n",
    " 18          0       3  31.000000      1      0   18.0000     0   \n",
    " 19          1       3  29.699118      0      0    7.2250     0   \n",
    " 20          0       2  35.000000      0      0   26.0000     1   \n",
    " 21          1       2  34.000000      0      0   13.0000     1   \n",
    " 22          1       3  15.000000      0      0    8.0292     0   \n",
    " 23          1       1  28.000000      0      0   35.5000     1   \n",
    " 24          0       3   8.000000      3      1   21.0750     0   \n",
    " 25          1       3  38.000000      1      5   31.3875     0   \n",
    " 26          0       3  29.699118      0      0    7.2250     1   \n",
    " 27          0       1  19.000000      3      2  263.0000     1   \n",
    " 28          1       3  29.699118      0      0    7.8792     0   \n",
    " 29          0       3  29.699118      0      0    7.8958     1   \n",
    " ..        ...     ...        ...    ...    ...       ...   ...   \n",
    " 861         0       2  21.000000      1      0   11.5000     1   \n",
    " 862         1       1  48.000000      0      0   25.9292     0   \n",
    " 863         0       3  29.699118      8      2   69.5500     0   \n",
    " 864         0       2  24.000000      0      0   13.0000     1   \n",
    " 865         1       2  42.000000      0      0   13.0000     0   \n",
    " 866         1       2  27.000000      1      0   13.8583     0   \n",
    " 867         0       1  31.000000      0      0   50.4958     1   \n",
    " 868         0       3  29.699118      0      0    9.5000     1   \n",
    " 869         1       3   4.000000      1      1   11.1333     1   \n",
    " 870         0       3  26.000000      0      0    7.8958     1   \n",
    " 871         1       1  47.000000      1      1   52.5542     0   \n",
    " 872         0       1  33.000000      0      0    5.0000     1   \n",
    " 873         0       3  47.000000      0      0    9.0000     1   \n",
    " 874         1       2  28.000000      1      0   24.0000     0   \n",
    " 875         1       3  15.000000      0      0    7.2250     0   \n",
    " 876         0       3  20.000000      0      0    9.8458     1   \n",
    " 877         0       3  19.000000      0      0    7.8958     1   \n",
    " 878         0       3  29.699118      0      0    7.8958     1   \n",
    " 879         1       1  56.000000      0      1   83.1583     0   \n",
    " 880         1       2  25.000000      0      1   26.0000     0   \n",
    " 881         0       3  33.000000      0      0    7.8958     1   \n",
    " 882         0       3  22.000000      0      0   10.5167     0   \n",
    " 883         0       2  28.000000      0      0   10.5000     1   \n",
    " 884         0       3  25.000000      0      0    7.0500     1   \n",
    " 885         0       3  39.000000      0      5   29.1250     0   \n",
    " 886         0       2  27.000000      0      0   13.0000     1   \n",
    " 887         1       1  19.000000      0      0   30.0000     0   \n",
    " 888         0       3  29.699118      1      2   23.4500     0   \n",
    " 889         1       1  26.000000      0      0   30.0000     1   \n",
    " 890         0       3  32.000000      0      0    7.7500     1   \n",
    " \n",
    "     age_was_missing  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
    " 0             False                        0                         0   \n",
    " 1             False                        1                         0   \n",
    " 2             False                        0                         0   \n",
    " 3             False                        0                         0   \n",
    " 4             False                        0                         0   \n",
    " 5              True                        0                         1   \n",
    " 6             False                        0                         0   \n",
    " 7             False                        0                         0   \n",
    " 8             False                        0                         0   \n",
    " 9             False                        1                         0   \n",
    " 10            False                        0                         0   \n",
    " 11            False                        0                         0   \n",
    " 12            False                        0                         0   \n",
    " 13            False                        0                         0   \n",
    " 14            False                        0                         0   \n",
    " 15            False                        0                         0   \n",
    " 16            False                        0                         1   \n",
    " 17             True                        0                         0   \n",
    " 18            False                        0                         0   \n",
    " 19             True                        1                         0   \n",
    " 20            False                        0                         0   \n",
    " 21            False                        0                         0   \n",
    " 22            False                        0                         1   \n",
    " 23            False                        0                         0   \n",
    " 24            False                        0                         0   \n",
    " 25            False                        0                         0   \n",
    " 26             True                        1                         0   \n",
    " 27            False                        0                         0   \n",
    " 28             True                        0                         1   \n",
    " 29             True                        0                         0   \n",
    " ..              ...                      ...                       ...   \n",
    " 861           False                        0                         0   \n",
    " 862           False                        0                         0   \n",
    " 863            True                        0                         0   \n",
    " 864           False                        0                         0   \n",
    " 865           False                        0                         0   \n",
    " 866           False                        1                         0   \n",
    " 867           False                        0                         0   \n",
    " 868            True                        0                         0   \n",
    " 869           False                        0                         0   \n",
    " 870           False                        0                         0   \n",
    " 871           False                        0                         0   \n",
    " 872           False                        0                         0   \n",
    " 873           False                        0                         0   \n",
    " 874           False                        1                         0   \n",
    " 875           False                        1                         0   \n",
    " 876           False                        0                         0   \n",
    " 877           False                        0                         0   \n",
    " 878            True                        0                         0   \n",
    " 879           False                        1                         0   \n",
    " 880           False                        0                         0   \n",
    " 881           False                        0                         0   \n",
    " 882           False                        0                         0   \n",
    " 883           False                        0                         0   \n",
    " 884           False                        0                         0   \n",
    " 885           False                        0                         1   \n",
    " 886           False                        0                         0   \n",
    " 887           False                        0                         0   \n",
    " 888            True                        0                         0   \n",
    " 889           False                        1                         0   \n",
    " 890           False                        0                         1   \n",
    " \n",
    "      embarked_from_southampton  \n",
    " 0                            1  \n",
    " 1                            0  \n",
    " 2                            1  \n",
    " 3                            1  \n",
    " 4                            1  \n",
    " 5                            0  \n",
    " 6                            1  \n",
    " 7                            1  \n",
    " 8                            1  \n",
    " 9                            0  \n",
    " 10                           1  \n",
    " 11                           1  \n",
    " 12                           1  \n",
    " 13                           1  \n",
    " 14                           1  \n",
    " 15                           1  \n",
    " 16                           0  \n",
    " 17                           1  \n",
    " 18                           1  \n",
    " 19                           0  \n",
    " 20                           1  \n",
    " 21                           1  \n",
    " 22                           0  \n",
    " 23                           1  \n",
    " 24                           1  \n",
    " 25                           1  \n",
    " 26                           0  \n",
    " 27                           1  \n",
    " 28                           0  \n",
    " 29                           1  \n",
    " ..                         ...  \n",
    " 861                          1  \n",
    " 862                          1  \n",
    " 863                          1  \n",
    " 864                          1  \n",
    " 865                          1  \n",
    " 866                          0  \n",
    " 867                          1  \n",
    " 868                          1  \n",
    " 869                          1  \n",
    " 870                          1  \n",
    " 871                          1  \n",
    " 872                          1  \n",
    " 873                          1  \n",
    " 874                          0  \n",
    " 875                          0  \n",
    " 876                          1  \n",
    " 877                          1  \n",
    " 878                          1  \n",
    " 879                          0  \n",
    " 880                          1  \n",
    " 881                          1  \n",
    " 882                          1  \n",
    " 883                          1  \n",
    " 884                          1  \n",
    " 885                          0  \n",
    " 886                          1  \n",
    " 887                          1  \n",
    " 888                          1  \n",
    " 889                          0  \n",
    " 890                          0  \n",
    " \n",
    " [891 rows x 11 columns],\n",
    " 'exit': <IPython.core.autocall.ExitAutocall at 0x7f55b790e400>,\n",
    " 'get_ipython': <bound method InteractiveShell.get_ipython of <IPython.core.interactiveshell.InteractiveShell object at 0x7f55b8453be0>>,\n",
    " 'keras': <module 'keras' from '/usr/local/lib/python3.5/dist-packages/keras/__init__.py'>,\n",
    " 'n_cols': 10,\n",
    " 'pd': <module 'pandas' from '/var/lib/python/site-packages/pandas/__init__.py'>,\n",
    " 'pred_data': array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "        [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "        [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "        [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "        [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "        [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "        [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "        [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "        [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "        [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "        [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "        [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "        [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "        [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "        [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "        [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "        [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "        [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "        [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "        [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "        [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "        [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "        [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "        [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "        [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "        [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "        [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "        [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "        [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "        [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "        [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "        [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "        [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "        [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "        [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "        [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "        [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "        [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "        [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "        [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "        [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "        [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "        [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "        [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "        [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]], dtype=object),\n",
    " 'predictors': array([[3, 22.0, 1, ..., 0, 0, 1],\n",
    "        [1, 38.0, 1, ..., 1, 0, 0],\n",
    "        [3, 26.0, 0, ..., 0, 0, 1],\n",
    "        ..., \n",
    "        [3, 31.0, 0, ..., 0, 0, 1],\n",
    "        [3, 30.0, 0, ..., 1, 0, 0],\n",
    "        [3, 30.0, 1, ..., 0, 0, 1]], dtype=object),\n",
    " 'quit': <IPython.core.autocall.ExitAutocall at 0x7f55b790e400>,\n",
    " 'target': array([[ 1.,  0.],\n",
    "        [ 0.,  1.],\n",
    "        [ 0.,  1.],\n",
    "        ..., \n",
    "        [ 0.,  1.],\n",
    "        [ 1.,  0.],\n",
    "        [ 1.,  0.]]),\n",
    " 'to_categorical': <function keras.utils.np_utils.to_categorical>,\n",
    " 'train_size': 800}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify, compile, and fit the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 1.9889 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 2.1642 - acc: 0.5878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 2.0926 - acc: 0.5850     \n",
    "Epoch 2/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.9758 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 1.2621 - acc: 0.5789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 1.2150 - acc: 0.5825     \n",
    "Epoch 3/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7413 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.9169 - acc: 0.6293\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.9013 - acc: 0.6225     \n",
    "Epoch 4/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.8295 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6976 - acc: 0.6335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.7067 - acc: 0.6350     \n",
    "Epoch 5/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7734 - acc: 0.5312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 0.7101 - acc: 0.6771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6920 - acc: 0.6775     \n",
    "Epoch 6/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7822 - acc: 0.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6394 - acc: 0.6705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6375 - acc: 0.6687     \n",
    "Epoch 7/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.5666 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "640/800 [=======================>......] - ETA: 0s - loss: 0.6170 - acc: 0.6734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6172 - acc: 0.6812     \n",
    "Epoch 8/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.6544 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "384/800 [=============>................] - ETA: 0s - loss: 0.6322 - acc: 0.6693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6181 - acc: 0.6837     \n",
    "Epoch 9/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.7973 - acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "704/800 [=========================>....] - ETA: 0s - loss: 0.6325 - acc: 0.6634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6183 - acc: 0.6737     \n",
    "Epoch 10/10\n",
    "\n",
    " 32/800 [>.............................] - ETA: 0s - loss: 0.5755 - acc: 0.6875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "672/800 [========================>.....] - ETA: 0s - loss: 0.6201 - acc: 0.6860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "800/800 [==============================] - 0s - loss: 0.6142 - acc: 0.6900     \n",
    "[ 0.18310049  0.39257559  0.90957892  0.43802381  0.19449668  0.16226816\n",
    "  0.04776059  0.36320636  0.13548838  0.59659457  0.21850654  0.24186897\n",
    "  0.14519456  0.54658085  0.1703662   0.09650292  0.3025246   0.50268072\n",
    "  0.06854767  0.53118235  0.66011029  0.21718189  0.05112241  0.28982982\n",
    "  0.58332694  0.18461925  0.58715367  0.68001962  0.19473749  0.59099054\n",
    "  0.44369563  0.56261528  0.1525811   0.25684106  0.31957388  0.68460965\n",
    "  0.2890453   0.19117369  0.59348351  0.42708611  0.29303789  0.36315283\n",
    "  0.45844418  0.09916571  0.334088    0.07964922  0.56860393  0.10694957\n",
    "  0.50475955  0.78340077  0.54485911  0.00884945  0.49051741  0.63139898\n",
    "  0.25409591  0.3637715   0.93466604  0.15533349  0.3382667   0.1525811\n",
    "  0.15455711  0.32354257  0.20677377  0.57712185  0.29516548  0.13900402\n",
    "  0.28757289  0.58676606  0.21134241  0.50062877  0.21862327  0.52192533\n",
    "  0.10416862  0.0638297   0.38311687  0.35487503  0.31126469  0.3045193\n",
    "  0.18921892  0.61726612  0.47175369  0.14496818  0.34283844  0.21893513\n",
    "  0.21895605  0.26417106  0.26272085  0.54475427  0.39032492  0.49731082\n",
    "  0.15875942]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
